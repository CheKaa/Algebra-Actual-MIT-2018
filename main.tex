\documentclass[10pt,a4paper,oneside]{book}
\usepackage[a4paper,includeheadfoot,top=10mm,bottom=10mm,left=10mm,right=10mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amsthm,amssymb,amscd,array}
\usepackage{latexsym}
\usepackage{multicol} % нумерция в нескольких колонках
\usepackage{graphicx} 
%\usepackage{pdfsync}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\usepackage{hyperref} % гиперссылки
\usepackage{cmap}       % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage{indentfirst}% Красная строка в первом абзаце
\usepackage{misccorr}
\usepackage{arydshln} % штрихованые линии в массивах
\usepackage{mathtools} % выравнивание в матрицах
\usepackage{ccaption}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={red!80!black}
}
% цвета для ссылок

\newtheorem{upr}{Упражнение}
\newtheorem{predl}{Предложение}
\newtheorem{komment}{Комментарий}
\newtheorem{conj}{Гипотеза}
\newtheorem{notation}{Обозначение}


\theoremstyle{definition}
\newtheorem{kit}{Кит}
\newtheorem*{rem}{Замечание}
\newtheorem{zad}{Задача}
\newtheorem*{defn}{Определение}
\newtheorem*{fact}{Факт}
\newtheorem{thm}{Теорема}
\newtheorem*{thmm}{Теорема}
\newtheorem{lem}{Лемма}
\newtheorem{cor}{Следствие}
\newtheorem{utvr}{Утверждение}




\renewcommand{\proofname}{Доказательство}
\renewcommand{\mod}{\,\operatorname{mod}\,}
\renewcommand{\Re}{\operatorname{Re}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ovl}{\overline}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\K}{\operatorname{K_0}}
\newcommand{\witt}{\operatorname{W}}
\newcommand{\gw}{\operatorname{GW}}
\newcommand{\coh}{\operatorname{H}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\cl}{\operatorname{Cl}}
\newcommand{\Vol}{\operatorname{Vol}}
\newcommand\tgg{\mathop{\rm tg}\nolimits}
\newcommand\ccup{\mathop{\cup}}
\newcommand{\id}{\operatorname{Id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\chr}{\operatorname{char}}
\newcommand{\rk}{\operatorname{rank}}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\Ker}{Ker}
\newcommand{\im}{\operatorname{Im}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\tr}{\operatorname{Tr}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\orb}{\operatorname{\mathcal O}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\Out}{\operatorname{Out}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Adj}{\operatorname{Adj}}
\newcommand{\Disc}{\operatorname{Disc}}
\newcommand{\cnt}{\operatorname{cont}}

\newcommand{\di}{\mathop{\,\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}

\newcommand{\ndi}{\mathop{\not\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\nequiv}{\not \equiv}
\newcommand{\Nod}{\operatorname{\text{НОД}}}
\newcommand{\Nok}{\operatorname{\text{НОК}}}
\newcommand{\sgn}{\operatorname{sgn}}


\def\llq{\textquotedblleft} 
\def\rrq{\textquotedblright} 
\def\exm{\noindent {\bf Примеры:}}


\def\Cb{\ovl{C}}
\def\ffi{\varphi}
\def\pa{\partial}
\def\V{\bf V}
\def\La{\Lambda}
\def\eps{\varepsilon}
\def\del{\delta}
\def\Del{\Delta}
\def\A{\EuScript{A}}
\def\lan{\left\langle }
\def\ran{\right\rangle}
\def\bar{\begin{array}}
\def\ear{\end{array}}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\thrm{\begin{thm}}
\def\ethrm{\end{thm}}
\def\dfn{\begin{defn}}
\def\edfn{\end{defn}}
\def\lm{\begin{lem}}
\def\elm{\end{lem}}
\def\zd{\begin{zad}}
\def\ezd{\end{zad}}
\def\prdl{\begin{predl}}
\def\eprdl{\end{predl}}
\def\crl{\begin{cor}}
\def\ecrl{\end{cor}}
\def\rm{\begin{rem}}
\def\erm{\end{rem}}
\def\fct{\begin{fact}}
\def\efct{\end{fact}}
\def\enm{\begin{enumerate}}
\def\eenm{\end{enumerate}}
\def\pmat{\begin{pmatrix}}
\def\epmat{\end{pmatrix}}
\def\utv{\begin{utvr}}
\def\eutv{\end{utvr}}
\def\bupr{\begin{upr}}
\def\eupr{\end{upr}}

\frenchspacing
\righthyphenmin=2
%\usepackage{floatflt}
\captiondelim{. }





\begin{document}

\title{Конспект. Осень 2018}
\date{}
\author{}
\maketitle
\tableofcontents

\chapter{Полилинейная алгебра}
\section{Кватернионы}

Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим подпространство в алгебре матриц $M_2(\mb C)$ вида
$$\mb H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\mb C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\mb H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jjk=-i=-kj.$$ Таким образом $\mb H$ образует ассоциативную алгебру размерности 4 над $\mb R$.
 
\dfn[Алгебра кватернионов] $\mb H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно их и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\mb C$-алгебры.
\ezd



Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \mb R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm


\dfn[Сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и мнимую или векторную часть $v=\Im x= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn

\dfn[Норма кватерниона] Определим норму кватерниона как $$||x||=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \mb H$, то $x^{-1}=\frac{\ovl{x}}{||x||^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\mb R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. Мы не будем обсуждать неассоциатные алгебры в связи с тем, что им находится применение либо внутри физических дисциплин, либо внутри самой математики. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $||x||=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $||xy||=||x||||y||$. В частности, $||x^{-1}||=||x||^{-1}$.
\elm

\crl[Сумма четырёх квадратов] В любом коммутативном кольце произведение $(a^2+b^2+c^2+d^2)(e^2+f^2+g^2+h^2)$ снова есть сумма четырёх квадратов.
\ecrl

Теперь легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{||y||^2||x||^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{||xy||^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.


\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \mb R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -||u||^2v+ \lan u,v\ran u$$
2) $|| [u,v]||= ||u||||v|||\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -||u||^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь 
$$||[u,v]||^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)= ||u||^2||v||^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=||u||^2||v||^2 - \lan u,v\ran^2= ||u||^2||v||^2(1-\cos^2 \ffi)$$
\endproof

\dfn Обозначим за $\mb H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\mb H_{1}\to \GL_3(\mb R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\mb R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \mb H \to \mb H$ вида $y \to xyx^{-1}$ Прежде всего покажем, что мы получили ортогональное преобразование $\mb R^4$. Имеем
 $$||xvx^{-1}||=||v||.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\mb R^3$. Таким образом $L_x$ ограничивается на $\mb R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. 
$$xux^{-1}=(a+bv)u(a-bv)= (a+bv)(au-[u,bv])=a^2u -ab[u,v]+ab[v,u]- b^2[v,[v,u]]=(a^2-b^2)u-2ab[u,v]$$
$$x[u,v]x^{-1}=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=a^2[u,v]+abu-ab[u,v]v-b^2uv=(a^2-b^2)[u,v]+2abu $$
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\mb R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь некоторое применение кватернионов.



\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами. Для этого заметим, что с каждым самосопряжённым оператором $L$ на евклидовом пространстве можно связать билинейную симметричную  форму $\lan x,Ly\ran$ или, что эквивалентно, квадратичную форму $\lan x,Lx\ran$. Безусловно по квадратичной форме можно обратно восстановить оператор. 

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти $$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \frac{\lan L^*Lx,x\ran}{\|x\|^2}}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум квадратичной формы на сфере.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
 где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$ \lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $U\cap W=\{0\}$, так как на $W$ форма принимает значения меньше или равные $\lambda_k$, а на $U$ -- строго большие. Однако $\dim W=n-k+1$. Приходим к противоречию с подсчётом размерности пересечения. 
\endproof


Введём определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базиc $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm

\crl Пусть $U$ некоторое подпространство, а $q(x)=x^{\top} Ax$. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$ упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
 $$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$ Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 








\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in V$, где $V$ -- это евклидово пространство размерности $n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<Наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=L+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=L+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее.

Действительно, выпишем условие $\sum_{i=1}^s \rho(x_i-a_0, L)^2=\sum ||pr_{L^{\bot}} (x_i-a_0)||^2$  минимально.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{L^{\bot}} x_i + 2 pr_{L^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $L^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $L$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетовряют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $L$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, L)^2=\sum_{i=1}^s ||pr_{L^{\bot}} (x_i)||^2.$$

Воспользуемся тем, что $||x||^2=||pr_L x||^2+||pr_{L^{\bot}} x||^2$ или $||x||^2-||pr_L x||^2=||pr_{L^{\bot}} x||^2$. Тогда получаем, что
$$\sum_{i=1}^s ||pr_{L^{\bot}}(x_i)||^2=\sum_{i=1}^s ||x_i||^2-||pr_L x_i||^2$$
должно быть минимально. Тогда сумма $\sum_i ||pr_L x_i||^2$ должна быть максимальна.


Для того чтобы посчитать проекцию выберем в $L$ ортонормированный базис $u_1,\dots,u_k$. Тогда перепишем
$$\sum_{i=1}^s ||pr_L x_i||^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся какой. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d=Xu_j$ состоит из скалярных произведений  $\lan v_i, u_j\ran$. Тогда выражение $\lan d,d\ran = (u_jX^{\top})Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2$, то есть как раз тому, что участвует в нашей сумме. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо минимизировать выражение
$$\sum_{j=1}^k q(u_j).$$



Таким образом мы ищем максимум $\Tr q_{L}$ по всем подпространствам $L$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. 
Сформулируем теперь ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно определённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $L=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. В частности, в методе главных компонент возникает матрица $X^{\top}X$ и её собственные числа. Про них поговорим подробнее.

\dfn[Сингулярные значения] Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда сингулярными значениями $A$ называются числа $\sigma_i=\sqrt{d_i}$, где $d_i>0$ -- положительные собственные числа оператора $A^*A \colon V \to V$. Если же говорить на языке матриц, то для матрицы $X$ её сингулярными значениями будут корни из собственных чисел $X^{\top}X$. 
\edfn

На самом деле мы не обсуждали определение сопряжённого линейного отображения, а только сопряжённого оператора. Напишу немного об этом.

\dfn Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда сопряжённым отображением к $A$, называется такое линейное отображение $A^{*}$, что $\lan A^*x,y\ran = \lan x,Ay\ran$ для всех $x\in V$ и $y \in U$.
\edfn

\thrm Сопряжённое линейное отображение единственно. Более того, если в $U$ и $V$ выбрать ортонормированные базисы, то матрица сопряжённого отображения в этих базисах будет равна транспонированной матрице исходного.
\proof Достаточно доказать последнюю часть, чтобы показать единственность и существование. Выберем ортонормированные базисы в $U$ и $V$ -- $u_j$ и $v_i$. Обозначим матрицу $A$ в этом базисе за $X$, а кандидата на $A^*$ за $Y$. Тогда для равенства из определения сопряжённости необходимо и достаточно, его выполения на базисных. Иными словами необходимо и достаточно, чтобы $\lan X^{*}e_i,e_j\ran=\lan e_i,Xe_j\ran$. Но первая часть даёт $X^{*}_{ji}$, а вторая -- $X_{ij}$. Итого необходимо и достаточно, чтобы $X^{*}=X^{\top}$.   
\endproof
\ethrm

Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений.


\thrm[SVD разложение] Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $A$ имеет вид 
$$\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $A$, числа $\Sigma=\sigma_1, \dots, \sigma_r$ её сингулярные значения.
На языке матриц это означает, что для любой матрицы $X \in M_{m\times n}$ существуют матрицы $L$ -- размера $m$ и $R$ -- размера $n$,  что
$$X= L \Sigma R,$$
 с теми же условиями на $r$ и $\sigma_i$.
 
\proof Рассмотрим оператор $B = A^{*}A$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Ae_i \in U$. Они ортогональны. Действительно
$$\lan Ae_i, Ae_j\ran = \lan A^{*}Ae_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $||e_i||^2=d_i$. Возьмём 
$$f_i=\frac{Ae_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Ae_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $A$
Напоследок осталось решить вопрос, как выглядит матрица $R$. В нашей конструкции матрица $R$ есть матрица замены из стандартного базиса в базис из собственных векторов $e_i$ матрицы $X^{\top}X$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $R=C^{-1}$, но $C$ ортогональна и поэтому можно написать $R=C^{T}$, то есть строки $R$ -- собственные вектора $X^{\top}X$. Часто эти вектора называют правыми сингулярными векторами $X$.

\endproof
\ethrm

\zd Получите аналогичное описание для $L$. Покажите так же, что ничего кроме $\Sigma$ в аналогичном разложении получится не может.
\ezd

SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $L$, то получится матрица ранга $k$ или меньше. Эта матрица будет ближайшей к исходной в смысле вот такой вот матричной нормы, называемой, нормой Фробениуса 
$$||X||_F=\sqrt{\sum_{i,j} a_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$
Таким образом нахождение проекций точек можно переформулировать, как нахождение ближайшей к матрице $X$ матрице ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение

\thrm Пусть $X\in M_{m\times n}(\mb R)$. И SVD-разложение $X$ имеет вид $X=L\Sigma R$, где на диагонали $\Sigma$ стоят $\sigma_1,\dots,\sigma_r$ и нули. Тогда наилучшим приближением ранга $k$ в смысле нормы Фробениуса к матрице $X$ будет матрица $X^{(k)}=L\Sigma^{(k)}R$, где на диагонали $\Sigma^{(k)}$ стоят $\sigma_1,\dots,\sigma_{k}$ и нули.
\proof Для того, чтобы найти матрицу $X^{(k)}$ необходимо спроецировать строки $X$ на подпространство $L=\lan v_1^{\top},\dots,v_k^{\top}\ran$, где $v_i$ ортонормированный базис из собственных векторов $X^{\top}X$.  Вспомним, что строки $R$ есть $v_i^{\top}$. Для того, чтобы спроецировать одну строку $a$ на пространство $V^{(k)}$ необходимо вычислить сумму $\sum_{i=1}^k (av_i)v_i^{\top}$. Применив это целиком к матрице $X=L\Sigma R$ получим 
$$X^{(k)}=\sum_{i=1}^k Xv_iv_i^{\top}=L\Sigma \left(\sum_{i=1}^k Rv_iv_i^{\top}\right).$$
Вычислим последнюю сумму. Эта сумма считает проекции строк $R$ на $L$. Но первые $k$ строк лежат в $L$, а остальные ортогональны $L$. Итого имеем
$$R^{(k)}=\sum_{i=1}^k Rv_iv_i^{\top} = \pmat v_1^{\top} \\ \vdots \\ v_k^{\top} \\ 0 \\ \vdots \\ 0 \epmat.$$
Осталось заметить, что $\Sigma^{(k)} R= \Sigma^{(k)}R^{(k)}=\Sigma R^{(k)}$.
\endproof
\ethrm 

SVD-разложение используется в разных задачах, в том числе и для сжатия изображений.  Для простоты рассмотрим случай квадратного $n \times n$ чёрно белого изображения. Сделаем из него вещественную матрицу $X$ размера $n \times n$ и найдём SVD-разложение $L \Sigma K$. Тогда приближение $X^{(k)}$ задаётся $L\Sigma^{(k)}R$. Однако, как мы уже заметили, вместо матрицы $R$ можно взять матрицу $R^{(k)}$. Аналогично вместо $L$ можно взять $L^{(k)}$ -- выкинув из $L$ последние $n-k$ столбцов. Для хранения матрицы $\Sigma^{(k)}$ нужно $k$ параметров, для матриц $L^{(k)}$ и $R^{(k)}$ по $kn$ параметров. Итого нужно $2kn+k$ параметров. Однако чтобы не хранить отдельно $\Sigma$ её можно домножить на $L$ и хранить $L\Sigma$. В таком случае необходимо $2kn$ параметров. При $k<\frac{n}{2}$ это даёт эффект сжатия. 

Однако, это не предел. Посмотрим, сколько параметров нужно, чтобы задать $X$ -- матрицу ранга $k$. Пусть главный минор размера $k$ матрицы $X$ не ноль (априори мы знаем, что какой-то минор такого размера не ноль). Тогда для $j$-ой строки матрицы, начиная с номера $j \geq k+1$ есть набор чисел $a_{1,j},\dots,a_{k,j}$, которые есть коэффициенты в линейной комбинации дающей из первых строк $j$-ую. Аналогично для столбцов. Такой набор данных задаётся $k^2+ 2k(n-k)=2kn-k^2$ параметрами. Осталось заметить, что всегда $2kn - k^2\leq n^2$ так как $0\leq n^2-2kn+k^2=(n-k)^2$. Если невырожденным оказался не главный минор, то дополнительно нужно задать $2k$ дискретных параметров задающий номера строк и столбцов невырожденного минора.



\section*{Дополнительно: поиск угла между подпространствами}
Попробуем решить другую задачу -- определить, что такое угол угол между подпространствами и понять, как его найти. 

\dfn
Пусть $U$ и $W$ два подпространства  евклидового пространства. Определим косинус угла между ними как 
$$\cos \angle U,W= \sup_{\substack{ x\in U\\ y\in V}} \frac{\lan x,y\ran}{||x|| ||y||}.$$
\edfn

\rm Если два подпространства пересекаются, то угол между ними по этому определению равен 0. Если хочется, чтобы угол не был равен 0 для неравных пространств, то разумно посмотреть ортогональные дополнения $U$ и $W$ к пересечению $U\cap W$ и посчитать угол между ортогональными дополнениями. 
\erm

\rm Можно переписать выражение для косинуса как $$\cos \angle U,W= \sup_{ x\in U} \cos \angle x, V = \sup_{x\in U} \frac{||pr_V x||}{||x||}= \sqrt{ \sup_{\substack{x\in U\\ ||x||=1}} ||pr_V x||^2} .$$
\erm

С последним выражением легко работать, потому что $||pr_V x||^2$ -- это квадратичная форма на $U$.

Допустим мы хотим максимизировать указанное выражение. Выберем на $U$ ортонормированный базис. Посчитаем матрицу формы $||pr_V x||^2$ по формуле 
$$a_{ij}= \lan pr_V u_i, pr_V u_j\ran.$$

Тогда максимум выражения под корнем равен $\lambda_{max}$ -- максимальному собственному числу $A$ и достигается на соответствующем векторе $v_{max}$. Ответ: $\sqrt{\lambda_{max}}$

\rm Получилось, что косинус угла -- это норма оператора проекции с одного подпространства на другое. Это не совсем то, что нас интересует в общем случае, например для гиперплоскостей. Дело в том, что для них ответ всегда 1, так как гиперплоскости заведомо пресекаются по подпространство ненулевой размерности (кроме как на плоскости). Вектора из пересечения проецируются в себя,что даёт норму равную единице. В этом случае стоит заменить оба подпространства на их ортогональные дополнения к пересечению. Это тоже самое, что найти первое неединичной собственное числа для формы  $\lan pr_V u_i, pr_V u_j\ran$ на $U$.
\erm




\section{Спектры графов}

\dfn
Для каждого графа $G$ можно построить  несколько  различных матриц, которые кодируют его структуру. Прежде всего это три квадратные матрицы  размера $n\times n$, где $n$ -- это число вершин $G$. 
Первая -- матрица смежности  $A(G)$, которая полностью определяет граф $G$
$$a_{ij}=\begin{cases} 1, \text{ если вершины $i$ и $j$ соединены ребром}\\
0, \text{ иначе }
\end{cases}.$$

Так же нам уже встречалась матрица случайного блуждания  $P(G)$

$$P_{ij}=\begin{cases}
\frac{1}{d_j}, \text{ если есть ребро $j\to i$}\\
1, \text{ если из вершины не исходит рёбер} \\
0, \text{ иначе }
\end{cases}.$$
Кроме того, полезна бывает матрица инцидентности $B(G)$ размера $n\times m$, где $m$ -- число рёбер.
\edfn

В прошлом семестре мы с вами поняли, что для понимания того, как устроен предел последовательности $P^nv$, необходимо представлять себе как устроены собственные числа матрицы $P$. Прежде всего мы с вами понимали, что у матрицы $P$ есть собственное число 1. Однако встаёт несколько вопросов:\\
1) Какова кратность единицы, как собственного числа?\\
2) Есть ли другие собственные числа, равные единице по модулю?\\
3) Если $Pv=v$, то мы хотели бы интерпретировать $v$ как вектор весов для вершин графа. Верно ли, что $v$ можно выбрать положительным? Сколько таких независимых $v$?

Понятно, что в общем случае ответ на первые два вопроса <<нет>>.

\exm \\
1) Рассмотрим граф 
\begin{center}
\begin{tikzpicture}

\begin{comment}
\draw [fill] (0,0) circle [radius=0.05];
\draw [fill] (1,0.5) circle [radius=0.05];
\draw [fill] (1,-0.5) circle [radius=0.05];
\end{comment}

\node (A) at (0,0) {3};
\node (B) at (1,0.5) {1};
\node (C) at (1,-0.5) {2};
\path[->,font=\scriptsize,>=angle 60]
(A) edge (B)
(A) edge (C);
\end{tikzpicture}
\end{center}
У его матрицы $P$ очевидно есть два собственных вектора $(1,0,0)$ и $(0,1,0)$ с собственным числом 1.\\
2) Рассмотрим граф $C_n$ -- цикл длины $n$. Его спектр -- это корни степени $n$ из единицы.\\

Сейчас мы докажем, что при некоторых предположениях на матрицу для неё ответы на все три вопроса оказываются положительными. Эти предположения не будут выполнены для матриц $A(G)$ и  $P(G)$ непосредственно, однако мы тем не менее сможем извлечь пользу.

\dfn Назовём матрицу $A$ положительной (не путать с положительно определённым оператором), если все её элементы $A_{ij}$ строго положительны. Будем писать в этом случае $A>0$.
\edfn

\dfn Назовём матрицу  $A$ не отрицательной, если $A_{ij}\geq 0$. Обозначение $A \geq 0$.
\edfn

\thrm[Перрон, 1907] Если матрица $A$ положительна, то наибольшее по модулю собственное число $A$ единственное и является вещественным и положительным. Это собственное число не является кратным корнем характеристического многочлена. Собственный вектор для этого собственного числа положителен.
\ethrm
\proof Пусть $\lambda$ -- наибольшее по модулю собственное число и $Ax=\lambda x$. Можно считать, что $|\lambda|=1$. Тогда покажем, что $A|x|=|x|$.

Прежде всего мы имеем цепочку неравенств $|x|=|Ax|\leq |A||x|=A|x|$, где все неравенства подразумеваются покомпонентными. Обозначим за $z=A|x|$. Это вектор состоящий из положительных координат и рассмотрим вектор $y=z-x$. Вектор $y$ неотрицателен. При этом если $y=0$, то мы доказали то, что хотели. Предположим, что есть координата $y_i>0$. Тогда $Ay$ -- положительный вектор, то есть существует такое $\eps>0$, что $Ay>\eps z$. Распишем это неравенство: $Az - z= Az-A|x|> \eps z$ или же $\frac{A}{1+\eps}z>z$. Ввиду положительности правой и левой части мы без сомнений можем применить оператор $\frac{A^n}{(1+\eps)^n}$ к правой и левой части и получить верное неравенство. Итого имеем цепочку 
$$\frac{A^n}{(1+\eps)^n}z>\frac{A^{n-1}}{(1+\eps)^{n-1}}> \dots > z.$$
Но оператор $\frac{A}{1+\eps}$ имеет собственные числа по модулю меньшие 1 и поэтому, как мы знаем с прошлого семестра, предел левого выражения равен 0. Противоречие!

Итак, в частности, единица собственное число. Покажем теперь, что нет отличных от единицы собственных чисел. Пусть $\lambda$ собственное число $A$ с $|\lambda|=1$ и $x$ -- соответствующий собственный вектор. Тогда $A|x|=|x|=|Ax|$. Заметим, что все координаты $x$ отличны от нуля. Рассмотрим $i$-ую координату. Имеем $\sum A_{ij}|x_j|=x_i=|\sum A_{ij}x_j|$. Посмотрим на это равенство как на равенство нормы векторов в $\mb R^2$. Хорошо известно, что сумма норм больше или равна нормы суммы и равенство достигается тогда и только тогда, когда вектора сонаправлены. Итого координаты $x_i$ должны быть сонаправлены, но это означает, что $x=e^{i\ffi} |x|$ и следовательно $\lambda=1$. 

Покажем, что единица не кратный корень. Действительно, пусть есть два собственных вектора $x_1$ и $x_2$. Тогда подберём $c$, так что $x_1-cx_2$ имеет нулевую координату. Получаем противоречие, так как $|x_1-cx_2|$ неотрицательный вектор для 1, но при этом с нулевой координатой. Осталось разобрать случай, когда $x_2$ присоединён к $x_1>0$, то есть $Ax_2=x_2+x_1$. Тогда имеем $A^nx_2=x_2 +nx_1$. Это значит, что какие-то коэффициенты $A^n$ растут по крайней мере линейно по $n$. Но тогда и коэффициенты $A^nx_1=x_1$ тоже растут по крайней мере линейно, что очевидно не так.
\endproof

Так же бывает полезно ещё одно утверждение. 
\lm Пусть $A>0$, $\lambda$ -- максимальное по модулю собственное число. Если у матрицы $A$ есть собственный вектор $y\geq 0$, то $y$ собственный вектор для числа $\lambda$
\elm
\proof Рассмотрим матрицу $A^{\top}$. У неё есть положительный  собственный вектор $x$, соответсвующий собственному числу $\lambda$. Пусть $\mu$ -- собственное число для $y$. Тогда 
$$\lambda x^{\top}y= x^{\top}Ay=x^{\top}\mu y=\mu x^{\top}y.$$
Так как $x^{\top}y >0$, то $\lambda=\mu$.
\endproof


Вообще говоря матрица $P(G)$ имеет довольно много нулевых компонент. И строго говоря теорема Перрона не может быть верна для $P(G)$. Как же она может помочь? Для этого мы схитрим и немного поменяем задачу. А именно, рассмотрим матрицу $$P_{\alpha}(G)=(1-\alpha) P(G) + \alpha\tfrac{1}{n}J_n,$$
где $J_n$ -- матрица из одних единиц, а $\alpha \in (0,1)$. Тогда матрицы $P_{\alpha}(G)$ являются положительными. С точки зрения блуждающего пользователя это означает, что у него есть два режима -- первый, в котором он находится с вероятностью $1-\alpha$ -- это режим брожения по ссылкам, а второй режим -- переход на случайную страницу. Для матрицы $P_{\alpha}(G)$ выполнены условия теоремы и поэтому она имеет единственное не кратное максимальное собственное число, которое положительно и соответствующий собственный вектор положителен. Покажем, что это собственное число равно 1.

Для этого рассмотрим матрицу $P_{\alpha}(G)^{\top}$. У этой матрицы есть положительный собственный вектор $(1,\dots,1)$ с собственным числом 1. Но тогда это максимальное по модулю собственное число для $P_{\alpha}(G)^{\top}$ и следовательно для $P_{\alpha}(G)$. Изучая предел $P_{\alpha}(G)$ при $\alpha \to 0$ можно получить информацию и про исходную матрицу.

То, что у $P_{\alpha}(G)$ все собственные числа по модулю меньше единицы означает, что $P_{\alpha}(G)^kv \to cx$, при $k \to \infty$, где $x$ -- положительный вектор с собственным числом равным 1. Это позволяет приближённо найти $x$, что даёт желаемое распределение весов. Практически для этого можно взять $k\sim \log n$. Это позволяет заметно сэкономить на вычислениях по сравнению с теоретическим нахождением собственных векторов.


Теперь переключимся на основной случай, который будем рассматривать -- случай неориентированных графов. В этой ситуации особую роль играет матрица $A(G)$.


\dfn Спектр графа -- это спектр его матрицы смежности $A(G)$.
\edfn


Для начала разберёмся с оценками и свойствами собственных чисел матрицы смежности. Здесь нам пригодится теорема Перрона. 


\lm Пусть граф $G$ связен. Тогда его максимальное собственное число положительно, не кратно и соответствующий собственный вектор имеет положительные координаты. Более того, все собственные числа графа по модулю меньше чем максимальная степень $d_{max}$. Граф $G$ регулярен тогда и только тогда, когда $d_{max}$ -- это его собственное число.
\elm
\proof Прежде всего отметим, что все собственные числа $G$ вещественные и максимальное собственное число положительно так как $\Tr A(G)=0$. Рассмотрим теперь матрицу $(A+\eps I)^{n-1}$. Это положительная матрица. Действительно в $(A+\eps I)^{n-1}_{ij}$ входит слагаемое $\eps^{n-1-l}$, где $l$ -- длина пути между $i$ и $j$. То же можно сказать и про большую степень $A+\eps I$. Максимальное с.ч. $A$ соответствует максимальному с.ч. $(A+\eps I)^l$ по крайней мере, если $\eps$ очень большое. Но тогда соответствующий собственный вектор $v$ положителен и максимальное собственное число $A+\eps I$ и, следовательно, $A$ не кратно. Далее $v$ положительный собственный вектор для всех $(A+\eps I)^{l}$, откуда получаем, что максимальное с.ч. у всех $(A+\eps I)^{l}$ наибольшее по модулю $(\lambda_1+\eps)^l>|\lambda_i+\eps|^l$. Переходя к пределу при $\eps \to 0$ получаем, что $\lambda_1^l \geq |\lambda_i|^l$. Осталось извлечь корень.

Почему же $\lambda_1 \leq d_{max}$? Пусть $x$ -- собственный вектор для числа $\lambda_1$. Тогда $Ax=\lambda_1 x$. Посмотрим, насколько мог измениться $x$ при домножении на $A$. Рассмотрим максимальную координату $x_i$. Имеем $\lambda_1 x_i= \sum a_{ij}x_j\leq d_{max} x_i$.

Предположим, что $d_{max}$ собственное число. Тогда в указанном выше неравенстве достигается равенство, то есть $x_i=x_j$ для соседних вершин. Но это значит, что вектор $(1,\dots,1)$ собственный, что бывает только в случае регулярного графа. 
\endproof


\exm \\
1) Спектр полного графа $K_n$ равен $n-1$ , $-1, \dots,-1$.\\
2) Спектр цикла длины $n$ равен $2\cos(\frac{2\pi l}{n})$.\\
Попробуем разобрать ещё один пример:

\dfn Сильно регулярный граф с параметрами $n$, $k$, $\lambda$ и $\mu$ это $k$-регулярный граф на $n$ вершинах, такой что любые две смежные вершины имеют $\lambda$ соседей, а любые две несмежные -- $\mu$ соседей.
\edfn

\thrm Матрица сильно $k$-регулярного графа удовлетворяет соотношению $A^2+(\mu-\lambda)A + (\mu-k)E=\mu J$, где $J$ -- это матрица из одних единиц.
\ethrm
\proof Возведём матрицу $A$ в квадрат. Тогда, число общих соседей равно числу путей длины 2 из $i$ в $j$, то есть $A^2_{ij}$. Если $i,j$ связаны между собой, то $A^2_{ij}=\lambda$, если не связаны, то $A^2_{ij}=\mu$, а на диагонали стоит $k$. Прежде всего вычтем $kE$, что даст нули на диагонали. Вычтя $\lambda A$ получим нули в тех позициях, что соответствовали рёбрам. Теперь надо добавить $\mu A+ \mu E$, чтобы эти позиции заполнить мюшками и получить $\mu J$. 
\endproof

Эта теорема позволяет легко посчитать спектры 
\crl
 Граф Петерсена сильно регулярный. Его спектр 3, 1, 1, 1, 1, 1, -2, -2, -2, -2.
\ecrl



Посмотрим, какие свойства графа можно увидеть благодаря его спектру.

\lm  След степени матрицы смежности считает количество циклов (возможно с пересечениями). В частности, граф двудольный тогда и только тогда, когда его спектр симметричен. 
\elm



Попробуем понять, какую ещё информацию даёт спектр. Воспользовавшись следствием из теоремы Куранта-Фишера получаем:

\thrm Пусть $G$ -- граф на $n$ вершинах. Пусть $A$ -- симметричная матрица $n\times n$, такая, что $A_{ij}= 0$, если вершины не соединены ребром. Пусть $n_{+}$ и $n_{-}$ количество положительных и отрицательных собственных чисел $A$. Тогда размер независимого множества в $G$ не превосходит $\min(n-n_{+},n-n_{-})$.
\ethrm
\proof Действительно, если взять подпространство, натянутое на вершины из независимого множества, то ограничение формы $x^{\top}Ax$ будет нулевым. Такое бывает только на подпространстве размерности $n-n_{+}=n_{-}+n_0$ исходя из нижней оценки. Аналогично получаем второе неравенство.
\endproof

Однако это не единственная возможная оценка. Разберём пример, использующий понятие спектра, но основанный на совершенно других предположениях.



\thrm Пусть $G$ -- $k$-регулярный граф. Тогда размер максимального независимого множества в графе оценивается как $$\alpha(G)\leq -\frac{\lambda_n}{k-\lambda_n}.$$
\ethrm
\proof Рассмотрим характеристический вектор $v$ для независимого множества размера $\alpha$. Имеем $v^{\top}Av=0$ и при этом $v^{\top}v=\alpha$. Так как граф $k$-регулярный, то у него есть нормированный собственный вектор $u_1=\frac{1}{\sqrt{n}}(1,\dots,1)$. Тогда $\lan v,u_1\ran = \frac{\alpha}{\sqrt{n}}$. Разложим вектор $v=c_1u_1 + \dots + c_n u_n$ по ортонормированной системе собственных векторов. Тогда
$$0=v^{\top}Av=\sum c_i^2 \lambda_i= \lambda_1\frac{\alpha^2}{n}+ \sum_{i\geq 2} \lambda_i c_i^2\geq \lambda_1\frac{\alpha^2}{n}+ \lambda_n \sum_{i\geq 2} c_i^2.$$
Мы знаем, что $\sum c_i^2=\alpha$, откуда $\sum_{i\geq 2} c_i^2=\alpha - \frac{\alpha^2}{n}$. Итого 
$$0\geq \lambda_1\frac{\alpha^2}{n}+\lambda_n(\alpha- \frac{\alpha^2}{n})$$
Сокращая на $\alpha$ получаем 
$$(\lambda_1-\lambda_n)\frac{\alpha}{n}\leq -\lambda_n,$$
что, очевидно, эквивалентно нужному неравенству.
\endproof

Обе оценки дают для графа Петерсена $\alpha(G)\leq 4$, что является точной оценкой. Разберём ещё один пример теоремы, которая хотя и не является сверхсодержательной, показывает, как можно использовать спектр графа.

\thrm Рассмотрим граф $K_{10}$. Тогда его невозможно покрыть тремя копиями графа Петерсона.
\ethrm

\section{Тензоры}

В прошлом семестре мы подробно остановились на билинейных операциях и их связи с геометрией, но не разобрали в достаточной степени, что же происходит с общими полилинейными отображениями.

\dfn Пусть есть набор пространств $V_1, \dots,V_n$. Тогда их тензорным произведением называется пространство 
$V_1\otimes \dots \otimes V_n$ вместе с полилинейным отображением
$$i \colon V_1 \times \dots \times V_n \to V_1 \otimes \dots \otimes V_n,$$
удовлетворяющее условию что для любого билинейного отображения из $h\colon V_1\times \dots \times V_n \to U$ существует единственное линейное отображение 
$$\hat{h}\colon V_1\otimes \dots \otimes V_n \to U,$$
что 
$$\hat{h}\circ i=h.$$
Иными словами, отображение $i$ -- это <<универсальное>> полилинейное отображение.
\edfn 


\lm Если тензорное произведение существует, то оно единственно.
\elm


Однако, совершенно непонятно есть такое пространство или нет. Более того, самом его существование нам тоже не сильно поможет. Нам нужна конструкция этого пространства и понимание свойств этой конструкции.

\thrm Пусть $V_1,\dots,V_n$ -- набор векторных пространств. Тогда имеет место следующая конструкция тензорного произведения:
$$V_1 \otimes \dots \otimes V_n \cong K\lan V_1 \times \dots \times V_n \ran / Rel,$$
где $Rel$ -- это подпространство порождённое формальными суммами
$$(\dots, \lambda v_1+v_2, \dots) - \lambda (\dots,v_1, \dots) - (\dots, v_2, \dots).$$ 
\ethrm

\dfn Будем обозначать элемент $i(v_1,\dots,v_n)=v_1\otimes \dots \otimes v_n$. 
\edfn

Теперь необходимо посчитать что-то про тензорное произведение. Например, научиться считать размерность тензорного произведения и находить его базис.
\thrm Пусть $e_{i1},\dots,e_{in}$ базис $V_i$. Тогда $$e_{1j_1}\otimes \dots \otimes e_{nj_n}$$ базис $V_1 \otimes \dots \otimes V_n$.
\ethrm



Определим теперь тензорное произведение линейных отображений.

\dfn Пусть набор линейных отображений $f_i \colon U_i \to V_i$. Определим отображение $$f_1\otimes \dots \otimes f_n \colon \otimes U_i \to \otimes V_i$$ по  правилу $$(f_1\otimes \dots \otimes f_n) (u_1\otimes \dots \otimes u_n) = f(u_1)\otimes \dots \otimes f(u_n).$$
Отображение с таким свойством единственно.
\edfn

\rm Указанное отображение корректно задано. Действительно,  отображение $f_1\otimes \dots \otimes f_n$ должно быть продолжением отображения $(u_1,\dots,u_n) \to f(u_1) \otimes \dots \otimes f(u_n)$, которое очевидно полилинейно. Тогда такое продолжение существует и единственно по свойству тензорного произведения. 
\erm

\rm Пусть заданы наборы линейных отображений $f_i$ и $g_i$, так что определены композиции $f_i\circ g_i$. Тогда $$f_1\otimes \dots \otimes f_n \circ g_1\otimes \dots \otimes g_n=(f_1\circ g_1)\otimes \dots \otimes (f_n\circ g_n)$$
\erm 

А как устроена матрица тензорного произведения линейных отображений?


\lm Пусть $L_1 \colon V_1 \to U_1$, а $L_2 \colon V_2 \to U_2$. Пусть $e_1,\dots, e_{n_1}$ базис $V_1$,  $e_1',\dots, e_{n_2}'$ базис $V_2$,  и $f_1,\dots, f_{m_1}$ -- базис $U_1$, а $f_1',\dots, f_{m_2}'$ -- базис $U_2$. 
Упорядочим базисы тензорных произведений -- удобно это сделать, например, в лексикографическом порядке (номер первой координаты важнее).
Тогда матрица  $L_1\otimes L_2$  разобьётся на $n_1m_1$ блоков в каждом из которых будет стоять $ A_{ij} B$, где $i,j$ -- номер блока, а $A$ и $B$ матрицы $L_1$ и $L_2$ соответственно.
\elm

\dfn Такая матрица называется кронекеровым произведением матриц $A$ и $B$ и обозначается как $A\otimes B$.
\edfn

А что если стартовать с операторов, а не с линейных отображений?

\rm Если есть операторы $A\colon V \to V$ и $B \colon W \to W$. Тогда задан оператор $A\otimes B$ на $V\otimes W$. \erm

\lm У оператора $A\otimes B$ собственные числа -- это попарные произведения с.ч. для $A$ и $B$. 
\elm


 

\dfn[Произведение графов] Пусть $G$ и $H$ -- два графа(возможно ориентированных). Тогда их категорным произведением называется граф чьи вершины есть пары вершин $G$ и $H$ и ребро между парами $(u_1,v_1)$ и $(u_2,v_2)$ проводится только если есть рёбра $u_1 \to u_2$ и $v_1 \to v_2$. Декартовым произведением графов $G$ и $H$ называется граф на тех же вершинах с ребром между парами если $u_1=u_2$ и есть ребро $v_1\to v_2$ или, симметрично, $v_1=v_2$ и есть ребро $u_1 \to u_2$. Разумеется для неориентированных графов эта конструкция снова выдаёт неориентированный граф.
\edfn

\rm Матрица произведения графов -- это тензорное произведение матриц.
\erm

\crl Спектр категорного произведения графов состоит из всех возможных попарных произведений собственных чисел графов.
\ecrl

\zd Чему равен спектр декартового произведения графов?
\ezd


С понятием тензорного произведения связан ряд канонических отождествлений между разными на первый взгляд пространствами в духе изоморфизма $V \sim V^{**}$.

\thrm Имеют место следующие естественные изоморфизмы: 
$$(U \otimes V) \otimes W \cong U \otimes V \otimes W \cong U \otimes (V \otimes W)$$
$$ U \otimes V \cong V \otimes U $$
$$ \Hom (U,V) \cong V \otimes U^*$$
$$ \Hom (U\otimes V,  W) \cong \Hom (U, \Hom (V,W))$$
$$(U \otimes V)^{*} \cong U^{*}\otimes V^{*}$$
\ethrm


Дадим определение:

\dfn Тензором валентности $(p,q)$ на пространстве $V$ называется элемент пространства ${V^{*}}^{\otimes p} \otimes V^{\otimes q}$. Так же будем говорить, что такие элементы -- это $p$ раз ковариантные и $q$ раз контравариантные тензоры. Тензорами валентности $(0,0)$ называются элементы поля $K$ -- скаляры.
\edfn

Теперь я утверждаю, что более менее все встречавшиеся нам структуры на векторном пространстве $V$ являются тензорами.



\exm\\
1) Вектор $v\in V$ является 1 раз контравариантным тензором.\\
2) Элемент двойственного пространства $f \in V^{*}$ является 1 раз ковариантным тензором. Вообще ковариантными называют тензоры, которые соответствуют полилинейным формам на пространстве $V$. Это историческая традиция. Точнее:\\
3) Так как пространство ${V^{*}}^{\otimes p} \cong {V^{\otimes p}}^*\cong \Hom(V,\dots,V,K)$, то тензор валентности $(p,0)$ соответствует полилинейному отображению $V\times\dots \times V \to K$.\\
4) В частности, тензор валентности $(2,0)$ -- это билинейная форма.\\
5) Линейный оператор -- это элемент $\Hom(V,V)\cong V^{*}\otimes V$, то есть тензор валентности $(1,1)$.\\
6) Структура алгебры на $V$ (без требования ассоциативности) задаётся билинейным отображением $V \times V \to V$, то есть линейным отображением $V\otimes V \to V$ или же элементом $V^{*}\otimes V^* \otimes V$, то есть тензором типа $(2,1)$.\\

Как записать тензор в координатах? Выберем базис $e_1,\dots,e_n$ пространства $V$ и возьмём в двойственном пространстве двойственный базис $e^1,\dots,e^n$. Теперь построим базис тензорного произведения ${V^{*}}^{\otimes p}\otimes V^{\otimes q}$. Он имеет вид $e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}$. Тогда произвольный тензор $T$ валентности $(p,q)$ имеет вид 
$$ T= \sum_{\substack{i_1,\dots,i_q \in \ovl{1,n}\\ j_1,\dots,j_p \in \ovl{1,n}} } \,T_{j_1,\dots,j_p}^{i_1,\dots,i_q}\,\, e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}.$$
Элементы $T_{j_1,\dots,j_p}^{i_1,\dots,i_q}$ называются координатами тензора $T$.


Как меняются координаты тензора при замене базиса? Посмотрим сначала на случай тензоров типа $(1,0)$.

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из старого базиса в новый. Тогда матрица перехода из базиса $e^1,\dots,e^n$ в базис $\hat{e}^1,\dots,\hat{e}^n$ есть ${C^{\top}}^{-1}$.
\ethrm

Теперь мы можем разобраться с тензорами общего вида:

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из старого базиса в новый, а $D={C^{\top}}^{-1}$. Тогда координаты тензора $T$ в базисе $\hat{e}$ выражаются через старые координаты следующим образом:
$$\hat{T}_{j_1,\dots,j_p}^{i_1,\dots,i_q}=\sum_{\substack{i'_1,\dots,i'_q \in \ovl{1,n}\\ j'_1,\dots,j'_p \in \ovl{1,n}}} \,\,
\prod_{t\in \ovl{1,p}} D_{j_t,j'_t} \prod_{s\in \ovl{1,q}} C_{i_s,i'_s}  \,\,T_{j'_1,\dots,j'_p}^{i'_1,\dots,i'_q}.$$
\ethrm

Важность тензоров в теоретической физике обуславливается тем, что практически все физические объекты -- это тензоры. Точнее: с точки зрения теории отоносительности пространство-время это некоторое четырёхмерное многообразие $M$ (в двумерной ситуации подошла бы обычная сфера или тор). С каждой точкой $x$ этого многообразия связано касательное пространство в этой точке -- некоторое четырёхмерное пространство $T_x$. Представим себе, что в каждой точке пространства задана плотность вещества (на самом деле не так, но допустим) -- это даёт вам функцию $f \colon M \to \mb R$ -- скаляр в каждой точке, то есть тензор типа $(0,0)$. 

Направление движения материи можно задать взяв в каждой точке касательный вектор, то есть тензор ранга $(0,1)$ на $T_x$. Дальше, у каждого такого вектора можно считать его <<длину>> и углы между векторами. Для этого надо задать для каждой точки $x$ билинейную форму на касательном пространстве, то есть элемент $T_x^{(2,0)}$. И т.д.

Важно, что уравнения в физике не должны зависеть от выбора координат. Можно, конечно, писать какие-то уравнения при помощи координат тензоров и каждый раз проверят, что выбрав новые координаты уравнение будет того же вида. Однако, чем сложнее наука тем сложнее становятся проверки. Становится важно работать с тензорами не рассматривая их координаты. Для этого мы обсудим две операции с тензорами, которые легко можно понять не используя координаты. Начнём с простой -- умножение тензоров.

\dfn Рассмотрим пространства $V^{p,q}$ и $V^{p',q'}$. Тогда имеет место билинейное отображение $$V^{p,q}\times V^{p',q'} \to V^{p+p',q+q'},$$
заданное правилом 
$$(v^1\otimes\dots\otimes v^p\otimes u_1\otimes\dots \otimes u_q,\hat{v}^1\otimes\dots\otimes \hat{v}^{p'}\otimes \hat{u}_1\otimes\dots \otimes \hat{u}_{q'}) \to v^1\otimes\dots\otimes v^p\otimes \hat{v}^1\otimes\dots\otimes \hat{v}^{p'}\otimes u_1\otimes\dots \otimes u_q \otimes \hat{u}_1\otimes\dots \otimes \hat{u}_{q'}.$$
Такое умножение задаёт структуру ассоциативной алгебры на пространстве 
$$T(V)=\bigoplus_{p,q\geq 0} V^{p,q},$$
которое называется тензорной алгеброй пространства $V$.
\edfn

Посмотрим теперь на пространство $V^*\otimes V$. Определим из него каноническое, то есть не зависящее от выбора базиса отображение в $\mb K$. Действительно элементы этого пространства есть суммы тензоров вида $f\otimes v$. Сопоставим каждому такому тензору 
$$(f,v) \to f(v).$$
Такое сопоставление продолжается до линейного отображения $$Conv \colon V^*\otimes V \to K.$$
Что это за отображение в координатах? Тензор типа $(1,1)$ записывается в базисе как $T=\sum_{i,j} T_j^i e^j\otimes e_i$. Тогда $$Conv(T)=\sum_{i,j} T_j^i e^j(e_i)=\sum_i T^i_i.$$
Если вспомнить, что пространство $V^*\otimes V \cong \Hom(V,V)$, то указанное отображение становится просто отображением следа. Это ещё один способ доказать инвариантность следа. Обозначение $Conv$ взято благодаря слову <<convolution>>, то есть свёртка. Сейчас мы проделаем аналогичную конструкцию в более общем случае. 

\dfn Рассмотрим пространство $V^{p,q}$ и пару индексов $j\leq p$ и $i\leq q$. Тогда свёрткой по индексам $i,j$ называется линейное отображение 
$$Conv_{i,j} \colon V^{p,q}\to V^{p-1,q-1},$$
заданное по правилу 
$$ \dots \otimes f^j\otimes \dots \otimes v_i \to f^j(v_i)\otimes \dots.$$
Это одно из самых часто используемых понятий про тензоры. В координатном виде это просто сумма по совпадающим индексам в позиции $j$ и $i$. Понятно, что свёртку можно делать по одинаковым по размеру упорядоченным группам координат. Я не буду про это говорить дополнительно.
\edfn




\section{Внешняя и симметрическая алгебры}

В этом разделе мы будем рассматривать векторное пространство $V$ над полем характеристики $0$. Есть общая теория не только для полей, но и для произвольных колец, однако, даже базовые конструкции в этой теории сложнее и некоторые её утверждения просто неверны над полем ненулевой характеристики.

\dfn Определим пространство $\Lambda^k V$ как подпространство $V^{\otimes k}$. Это подпространство выделяется следующими условиями -- для любой перестановки из $\sigma \in S_k$ и любого тензора $a\in \Lambda^k V$ верно, что $a^{\sigma}=\sgn(\sigma)a$. Под $a^{\sigma}$ подразумевается действие перестановки $\sigma$ на тензор $a$ перестановкой его компонент. Аналогично определяется подпространство $\Sym^k V \leq V^{\otimes^k}$, чьи элементы удовлетворяют свойству: $a^{\sigma}=a$.
\edfn

\lm Имеет место проектор $Alt \colon V^{\otimes k} \to \Lambda^k V$ заданный формулой 
$$a \to \frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) a^{\sigma}.$$
Аналогично отображение  
$$S\colon a \to \frac{1}{k!} \sum_{\sigma \in S_k} a^{\sigma}$$
есть проектор на подпространство $\Sym^k V$.
\elm

\dfn Пусть $e_1,\dots, e_k$ набор элементов из $V$. Определим элементы $e_1\wedge \dots \wedge e_k \in \Lambda^k V$ как образы при проекции $e_1\otimes \dots \otimes e_k$.
\edfn

\thrm Пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$ образуют базис пространства $\Lambda^k V$. В частности размерность $\dim \Lambda^k V = C^k_n$.
\ethrm

\thrm Аналогично, пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы образы тензоров $e_{i_1}\otimes \dots \otimes e_{i_k}$, где $i_1\leq \dots \leq i_k$ образуют базис пространства $\Sym^k V$.
\ethrm

\dfn Определим $k$-ую внешнюю степень линейного отображения $L\colon V \to W$ -- отображение $\Lambda^{k} L  \colon \Lambda^k V \to \Lambda^k W$ заданное на тензорах по правилу $v_1\wedge \dots \wedge v_k \to L v_1 \wedge \dots \wedge L v_k$. 
\edfn

Для того, чтобы показать корректность такого определения покажем следующую теорему:

\thrm Рассмотрим отображение $g=Alt \circ i \colon V^{\times k} \to \Lambda^k(V)$. Тогда для любого полилинейного кососимметрического $h \colon V^{\times k} \to U$ существует единственное отображение $\hat{h} \colon \Lambda^k(V) \to U$, что $\hat{h} \circ g = h$.
\ethrm


\fct Полезно смотреть не на пространства $\Lambda^k (V)$ и $\Sym^k V$, а на пространства $\Lambda^k(V^*)$ и $\Sym^k(V^*)$, потому что они допускают привычную и наглядную интерпретацию --- их элементы это полилинейные функции со специальными свойствами.
\efct


\exm \\
1) Элемент $\Lambda^2(V^*)$ --- это просто кососимметрическая билинейная форма.\\
2) А элемент $\Sym^2 V^*$ -- это симметрическая билинейная форма или просто квадратичная форма.\\
3) Элемент $\Lambda^{\dim V} V^*$ -- это просто форма объёма на $V$.\\
4) Заметим, что продолжая аналогию с квадратичными формами, выбор базиса задаёт изоморфизм 
$$\Sym^k V^* \cong K[x_1,\dots, x_n]_{\deg =k}$$
с пространством однородных многочленов степени $k$ ($n$ -- размерность пространства). Последнее отображение устроено следующим образом -- элементу $a \in \Sym^k V^* $ сопоставим отображение, которое на векторе $v=x_1e_1+\dots+x_ne_n $ выдаёт $a(v,\dots,v)$. То есть 
$$a \to (v \to a(v,\dots,v)).$$
 Это будет однородный многочлен от координатных функций $x_1, \dots, x_n$. Осталось заметить, что проекция тензора $e^{i_1}\otimes \dots \otimes e^{i_k}$ после применения такой операции --- это многочлен $x_{i_1}\dots x_{i_k}$.
 
По аналогии с тензорной алгеброй мы хотим ввести умножение на кососимметричных тензорах. Есть два подхода: первый из них -- потребовать, чтобы пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходила в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. С таким подходом возникает вопрос о корректности. С другой стороны, такое умножение удобно вычислять. Второй подход -- связать это умножение с обычным умножением тензоров. Проблема возникает в следующем -- если $T_1$ и $T_2$ -- кососимметрические тензоры, то $T_1\otimes T_2$ вообще говоря не кососимметрический. Вот как мы с этим разберёмся

\thrm[Внешняя алгебра] Рассмотрим пространства $\Lambda(V)=\oplus_{k=0}^{\dim V} \Lambda^k(V)$ и введём на нём структуру ассоциативной алгебры по правилу $ f\wedge g= Alt(f\otimes g)$. Если $f\in \Lambda^p(V)$, а $g \in \Lambda^q(V)$, то $f\wedge g=(-1)^{pq}g \wedge f$. Такое свойство называется градуированной коммутативностью. Более того, пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходит при этом умножении в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. Такое умножение называется внешним произведением тензоров.
\proof Для этого удобно проверить тождество $Alt(Alt(T_1)\otimes T_2)= Alt(T_1\otimes T_2)= Alt(T_1 \otimes Alt(T_2))$, которое говорит, что внутри симметризации можно свободно симметризовать сомножители не боясь ничего поменять. Действительно
$$\frac{1}{k!}\sum_{\sigma \in S_{k}}\sgn(\sigma) Alt(T_1^{\sigma}\otimes T_2)=\frac{1}{k!}\sum_{\sigma \in S_{k}} \sgn^2(\sigma) Alt(T_1\otimes T_2)=Alt(T_1 \otimes T_2).$$
Аналогично получается второе равенство. Теперь видно, что 
$$Alt(v_1 \wedge \dots \wedge v_p \otimes u_1\wedge \dots \wedge u_q)=Alt(v_1 \otimes \dots \otimes v_p \otimes u_1\wedge \dots \wedge u_q)=Alt(v_1 \wedge \dots \wedge v_p \otimes u_1\otimes \dots \otimes u_q) =v_1 \otimes \dots \otimes v_p \wedge u_1\wedge \dots \wedge u_q .$$
Это показывает связь нашего определения усножения с ожидаемым определением. Ассоциативность теперь легко проверить на базисных элемнтах, как и градуированную коммутативность.
\endproof
\ethrm

Изначально, внешняя алгебра была нужна для <<исчисления подпроcтранств>> в пространстве $V$. А именно, подпространству $U\ leq V$ размерности $k$ можно сопоставить прямую $\Lambda^k U$ в $\Lambda^k V$. Более того, задав на $U$ форму объёма можно выбрать на этой прямой определённую точку. Такие объекты теперь можно перемножать и складывать, хотя в общем случае может получиться и объект, не соответствующий никакому подпространству. 

\thrm[Симметрическая алгебра] Рассмотрим пространство $\Sym(V)=\oplus_k \Sym^k(V)$. Тогда на нём можно ввести структуру ассоциативной коммутативной алгебры задав умножение как $ f*g= S(f\otimes g)$. Более того, указанная алгебра изоморфна алгебре многочленов.
\proof Очевидно, что операция билинейна. Осталось проверить ассоциативность, для чего можно ограничиться рассмотрением базисных элементов. Это будет полезно в дальнейшем. Найдём непосредственно произведение $e_{i_1}\dots e_{i_k}\cdot e_{j_1}\dots e_{j_l}$. Я утверждаю, что оно равно $e_{i_1}\dots e_{i_k} e_{j_1}\dots e_{j_l}$.
Для этого удобно проверить тождество $S(S(T_1)\otimes T_2)= S(T_1\otimes T_2)= S(T_1 \otimes S(T_2))$, которое говорить, что внутри симметризации можно свободно симметризовать сомножители не боясь ничего поменять. Действительно
$$\frac{1}{k!}\sum_{\sigma \in S_{k}} S(T_1^{\sigma}\otimes T_2)=\frac{1}{k!}\sum_{\sigma \in S_{k}} S(T_1\otimes T_2)=S(T_1 \otimes T_2).$$
Аналогично получается второе равенство.
Теперь $e_{i_1}\dots e_{i_k}\cdot e_{j_1}\dots e_{j_l}= S(e_{i_1}\otimes\dots \otimes e_{i_k}\otimes e_{j_1}\otimes \dots\otimes e_{j_l})=$
Очевидно, что умножение заданное таким правилом ассоциативно. Коммутитивно оно по свойству симметрической степени. На самом деле, сопоставляя тензору $e_{i_1}\dots e_{i_k}$ моном $x_{i_1}\dots x_{i_k}$ мы строим изоморфизм алгебры $\Sym V$ и $K[x_1,\dots,x_{\dim V}]$.
\endproof
\ethrm

Покажем способ применения внешней степени для доказательства тождеств про определители. Прежде всего пусть есть отображение $L \colon U \to V$, где $\dim U= \dim V = n$ и $A$ матрица $L$ в базисах $e_1,\dots e_n$ и $f_1,\dots,f_n$. Тогда $$\Lambda^k L(e_1\wedge \dots \wedge e_n) = \det A f_1 \wedge \dots \wedge f_n.$$
Из этого замечания уже легко получить мультипликативность определителя. Поступаю аналогично можно доказать более общую теорему:
 
\thrm[Формула Бине-Коши] Рассмотрим две матрицы $A\in M_{m\times n}(K)$ и $B\in M_{n\times m}(K)$. Пусть $m\leq n$. Тогда
$$\det(AB)=\sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=m}} \det A^{\Gamma} \det B_{\Gamma}.$$
\ethrm





\chapter{Многочлены}
Настала пора вернуться к кольцу многочленов. Но на этот раз мы поговорим подробно о кольце многочленов от $n$ переменных. Мы знаем, что кольцо многочленов $K[x]$ над полем $K$ является областью главных идеалов. Есть ли надежда сказать тоже самое про многочлены от двух переменных? 

Ответ на этот вопрос даёт следующий пример: рассмотрим идеал $\lan x,y \ran$ в кольце $K[x,y]$. Это максимальный идеал. Но как мы знаем, этот идеал нельзя породить одним элементом (см. отступление про модули в прошлом семестре). 

Однако мы интуитивно представляем то, что разложение многочленов на множители однозначно. Напомню, что математически такое свойство называлось факториальностью. Итак, наша ближайшая цель поговорить о факториальности колец многочленов. Однако, для приложений к теории чисел нам будет необходимо разработать теорию в достаточной общности, чтобы применить её и над $\mb Z$, а не только над полем. Заметим, что $\mb Z$ и поле $K$ являются факториальными кольцами. Мы покажем, что факториальность кольца влечёт факториальность кольца многочленов над ним, что полностью ответит на наши вопросы.



\section{Многочлены над факториальным кольцом}

Наша задача обсудить, что происходит с кольцом многочленов от многих переменных.


\lm[Гаусс] Пусть $R$ -- факториальное кольцо. Тогда любой простой элемент $p$ из $R$ остаётся простым в $R[x]$.
\proof
Теоретически удобно воспользоваться следующим соображением: чтобы показать, что элемент прост надо показать, что идеал $(p)$ в $R[x]$ прост, а для этого необходимо и достаточно установить, что $R[x]/(p)$ есть область целостности. Чему же равно $R[x]/(p)$? Я утверждаю, что оно равно $R/p[x]$. Действительно, из $R[x]$ есть отображение в $R/(p)[x]$, которое берёт все коэффициенты по модулю $p$. Очевидно, в его ядре лежат многочлены, все коэффициенты которых делятся на $p$, то есть многочлены кратные $p$ в $R[x]$. Но ровно они и образуют идеал $(p)$. Осталось заметить, что кольцо $R/p$ и вслед за ним кольцо $R/p[x]$ являются областями целостности.

У этого доказательства есть другая, более элементарная реинкарнация. А именно, формально нам надо доказать, что если произведение двух многочленов $f(x)g(x)$ делится на $p$ (то есть все коэффициенты кратны $p$), то тогда какой-то из них делится на $p$. Пусть это не так. Возьмём тогда у $f$ и у $g$ самые младшие коэффициенты, которые не делятся на $p$ --  $a_i$ и  $b_j$. Тогда посмотрим на коэффициент с номером $i+j$  в произведении. Он имеет вид $c_{i+j}= a_ib_j + \sum_{k \neq i} a_k b_{i+j -k}$. Я утверждаю, что $c_{i+j}$ не делится на $p$. Для этого заметим, что любое слагаемое в сумме делится на $p$, так как либо $k<i$ и тогда $a_i \di p$, либо $k>i$, то есть $i+j-k<j$ и следовательно $b_j \di p$. Противоречие с тем, что $c_{i+j}$ должен делиться на $p$.   
\endproof
\elm

\bupr Поясните, почему оба доказательства одинаковы.
\eupr

\dfn Пусть $f(x)$ -- многочлен над факториальным кольцом $R$. Тогда содержанием $f$ называется $\cnt(f)=\Nod (a_i)$, где $a_i$ коэффициенты $f$. 
\edfn

Тут есть некоторая вольность -- надо помнить, что наибольший общий делитель определём с точностью до обратимых множителей. Следующее следствие тоже называют леммой Гаусса.

\crl Если $f(x)=g(x)h(x)$, где $f,g,h \in R[x]$, то $\cnt(f)=\cnt(g)\cnt(h)$
\proof Для начала, упростим задачу, то есть сведём задачу к случаю $\cnt g= \cnt h =1$. Для этого надо рассмотреть многочлены $\frac{g}{\cnt g}$ и $\frac{h}{\cnt h}$. Их произведение есть $\frac{f}{\cnt{g}\cnt{h}}$ имеет содержание $\frac{\cnt f}{\cnt g \cnt h}$ и если показать его единичность, то мы добьёмся требуемого. Итак считаем, что $\cnt g= \cnt h=1$. Если $\cnt f$ не обратим, то $\cnt f \di p$, где $p$ простой элемент из $R$. Но тогда один из $g$ или $h$ делится на $p$ благодаря его простоте. 
\endproof
\ecrl



\lm Пусть для многочлена $f(x) \in R[x]$  имеет место разложение $f(x)=g(x)h(x)$, где  $g(x)h(x) \in Q(R)[x]$. Тогда существуют такая константа $c \in Q(R)$, что $cg \in R[x]$ и $c^{-1}h \in R[x]$, что означает, что $f(x)=cg(x)c^{-1}h(x)$ -- есть произведение двух многочленов из $R[x]$ пропорциональных исходным.
\proof
Рассмотрим несократимую запись для коэффициентов $g$ и $h$ и обозначим наибольший общий делитель их знаменателей за $d_1$ и $d_2$ соответственно. Тогда $d_1g$ и $d_2h$ лежат в $R[x]$. Имеет место равенство $d_1 d_2\cnt f = \cnt(d_1 g) \cnt(d_2h)$. Таким образом, правая часть делится на $d_1d_2$. На самом деле $\cnt(d_1 g) \di d_2$ так как $d_2$ и $\cnt(d_2h)$ взаимно просты. Аналогично $\cnt(d_2h) \di d_1$. Осталось взять в качестве $c= \frac{d_1}{d_2}$.

\endproof
\elm


\thrm Пусть $R$ -- факториальное кольцо. Тогда кольцо $R[x]$ факториально. Более того, имеет место следующее описание простых элементов кольца $R[x]$:\\
1)  $\cnt(f)=1$ и $f$ неприводим в $Q(R)[x]$.\\
2) $f=p \in R$ -- простой в $R$.
\proof 
Для начала покажем, что все указанные ситуации приводят к простым элементам в кольце $R[x]$ и что других простых и, более того, неприводимых не бывает.
Итак, пусть $f \in R[x]$ неприводим в $Q(R)[x]$. Если $gh\di f$, то это же верно над $Q(R)$ и, можно считать например, что $g\di f$ в $Q(R)[x]$. Тогда $g= fk$. Теперь можно домножить на подходящую константу $g= (cf) (c^{-1}k)$ чтобы получить равенство в $R[x]$. Заметим, что $c(c^{-1}k)$ из $R[x]$, что показывает, что $g \di f$ в $R[x]$. Второй случай полностью следует из леммы Гаусса.


Теперь покажем, что любой элемент раскладывается в произведение указанных простых. Для этого сначала разложим $f$ в $Q(R)[x]$ в произведение неприводимых $f=\prod g_i$, $g_i \in Q(R)[x]$. Далее сделаем из $g_i$ элементы $\hat{g}_i$ из $R[x]$ с $cont(g_i)=1$, что $f=a\prod \hat{g}_i$. Заметим, что $a=cont(f)$ и, следовательно, лежит в $R$. Итого $f=a\prod \hat{g}_i$, где $ 0 \neq c \in R$. Осталось разложить $c$.

Осталось показать единственность. Это следует лишь из того, что у нас есть разложение на простые. Действительно, если $f=\prod p_i=\prod q_i$, то $p_i \di \prod q_i$ и благодаря простоте делит скажем $q_i$. Но тогда $p_ih=q_i$, откуда, благодаря неприводимости $q_i$ получаем, что $h$ обратим, то есть, что $p_i \sim q_i$. Тогда можно сократить на $p_i$ и продолжить по индукции. Отсутствие простых отличного от указанных типов следует теперь из единственности разложения.
\endproof
\ethrm





\section{Признаки неприводимости для многочленов}

Теперь наша задача поговорить про неприводимость многочленов над целыми числами или над $\mb Q$. 
Прежде всего отметим, что обе задачи тесно связаны. А именно, если взять многочлен с рациональными коэффициентами, то домножив его на подходящую рациональную константу мы получим многочлен с целыми коэффициентами и содержанием 1, который по доказанному ранее неприводим тогда и только тогда, когда неприводим исходный. Обратно, неприводимость целочисленных многочленов интересна только в случае, когда содержание этих многочленов равно единице. А в этом случае это эквивалентно рациональной неприводимости. Однако все теоремы я буду формулировать в общем контексте.



\thrm[Редукционный критерий] Пусть $R$ факториальное кольцо, $f \in  R[x]$ многочлен, а $p$ -- простой элемент. Тогда, если старший коэффициент $f$ не делится на $p$ и $\ovl{f}$ неприводим в кольце $R/p[x]$, то он неприводим над $Q(R)$. 
\proof Прежде всего перейдём от многочлена $f$ к $\frac{f}{cont(f)}$ с содержанием равным 1. Достаточно доказать неприводимость  последнего над $Q(R)$, которая эквивалентна его неприводимости над $R$. Итак пусть $cont(f)=1$ и пусть $f=gh$, где $g,h$ --  не константы. Старшие коэффициенты $g$ и $h$ тоже не делятся на $p$. Имеем $\ovl{f}= \ovl{g}\ovl{h}$ и $\deg g = \deg \ovl{g}$ и $\deg h = \deg \ovl{h}$, что даёт нетривиальное разложение $\ovl{f}$ и приводит к противоречию.
\endproof
\ethrm

Вот примеры о том, как пользоваться этим критерием и что не надо забывать про условие со старшим коэффициентом. 


\exm\\
1) Многочлен $x^3+x+1$ неприводим над $\mb F_2=\mb Z/2$, потому что у него нет корней. Следовательно многочлены $3x^3+8x^2+5x+7$ и скажем, $5x^3-4x^2+x+15$ неприводимы над $\mb Q$.\\
2) Рассмотрим многочлен $px^2+x$. Он приводим, но по модулю $p$ -- неприводим.\\
3) Критерий из теоремы сформулирован не в самом сильном виде. А именно, представим себе, например, что по модулю 2 многочлен степени пять разложился в произведение двух неприводимых степени 2 и 3, а по модулю 3 -- в виде произведения степени 4 и 1. Ясно, что он неприводим.\\
4) Не стоит забывать, что если многочлен неприводим над $\mb R$, то он так же неприводим над $\mb Q$. Это, правда, очень слабый критерий, но в комбинации с пунктом 3) может что-то дать.\\



Есть, однако, такие многочлены, которые неприводимы, но раскладываются по модулю любого простого. Например, $$x^4+1=(x-e^{\frac{i\pi}{8}})(x-e^{\frac{3i\pi}{8}})(x-e^{\frac{5i\pi}{8}})(x-e^{\frac{7i\pi}{8}})= (x^2+i)(x^2-i)=(x^2+\sqrt{2}x+1)(x^2-\sqrt{2}x+1)=(x^{2}+\sqrt{-2}x+1)(x^{2}-\sqrt{-2}x+1).$$ Он не имеет корней, а любые множители степени 2 имеют нерациональный коэффициент.

С другой стороны по любому простому модулю либо из $-1$, либо из $2$ либо из $-2$ извлекается корень.

Покажем теперь некоторый критерий неприводимости, который применим в случае, если разложение по модулю $p$ получилось неудачное. А именно, представим себе, что $f(x) \equiv x^{n} \mod p$. То есть развалился в произведение максимально возможного числа одинаковых множителей. Оказывается, что в этом случае неприводимость многочлена $f$ зависит от его класса по модулю $p^2$. Точнее:

\thrm[Признак Эйзенштейна] Пусть $R$ -- факториальное кольцо и $f(x)= a_0 + \dots + a_n x^n$. Если $a_n \ndi p$, все $a_i \di p$ $i<n$, но $a_0\ndi p^2$, то многочлен $f(x)$ неприводим.
\proof
Предположим, что $f=gh$. Заметим, что старшие коэффициенты $b_k$ и $c_l$ у $g$ и $h$ не делятся на $p$. Пусть так же $b_0 \ndi p$ у многочлена $g$. Рассмотрим самый младший коэффициент у $h$ не делящийся на $p$. Это точно не $c_0$. Пусть это $c_s$. Тогда $a_s=c_sb_0+c_{s-1}b_1+\dots+c_0b_s$. Все слагаемые, кроме первого делятся на $p$. Но тогда $a_s \ndi p$, что невозможно.
\endproof
\ethrm 








\end{document}