\documentclass[10pt,a4paper,oneside]{book}
\usepackage[a4paper,includeheadfoot,top=10mm,bottom=10mm,left=10mm,right=10mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amsthm,amssymb,amscd,array}
\usepackage{latexsym}
\usepackage{multicol} % нумерция в нескольких колонках
\usepackage{graphicx} 
%\usepackage{pdfsync}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\usepackage{hyperref} % гиперссылки
\usepackage{cmap}       % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage{indentfirst}% Красная строка в первом абзаце
\usepackage{misccorr}
\usepackage{arydshln} % штрихованые линии в массивах
\usepackage{mathtools} % выравнивание в матрицах
\usepackage{ccaption}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={red!80!black}
}
% цвета для ссылок

\newtheorem{upr}{Упражнение}
\newtheorem{predl}{Предложение}
\newtheorem{komment}{Комментарий}
\newtheorem{conj}{Гипотеза}
\newtheorem{notation}{Обозначение}


\theoremstyle{definition}
\newtheorem{kit}{Кит}
\newtheorem*{rem}{Замечание}
\newtheorem{zad}{Задача}
\newtheorem*{defn}{Определение}
\newtheorem*{fact}{Факт}
\newtheorem{thm}{Теорема}
\newtheorem*{thmm}{Теорема}
\newtheorem{lem}{Лемма}
\newtheorem{cor}{Следствие}



\renewcommand{\proofname}{Доказательство}
\renewcommand{\mod}{\,\operatorname{mod}\,}
\renewcommand{\Re}{\operatorname{Re}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ovl}{\overline}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\K}{\operatorname{K_0}}
\newcommand{\witt}{\operatorname{W}}
\newcommand{\gw}{\operatorname{GW}}
\newcommand{\coh}{\operatorname{H}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\cl}{\operatorname{Cl}}
\newcommand{\Vol}{\operatorname{Vol}}
\newcommand\tgg{\mathop{\rm tg}\nolimits}
\newcommand\ccup{\mathop{\cup}}
\newcommand{\id}{\operatorname{Id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\chr}{\operatorname{char}}
\newcommand{\rk}{\operatorname{rank}}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\Ker}{Ker}
\newcommand{\im}{\operatorname{Im}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\tr}{\operatorname{Tr}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\orb}{\operatorname{\mathcal O}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\Out}{\operatorname{Out}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Adj}{\operatorname{Adj}}
\newcommand{\Disc}{\operatorname{Disc}}
\newcommand{\cnt}{\operatorname{cont}}

\newcommand{\di}{\mathop{\,\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}

\newcommand{\ndi}{\mathop{\not\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\nequiv}{\not \equiv}
\newcommand{\Nod}{\operatorname{\text{НОД}}}
\newcommand{\Nok}{\operatorname{\text{НОК}}}
\newcommand{\sgn}{\operatorname{sgn}}


\def\llq{\textquotedblleft} 
\def\rrq{\textquotedblright} 
\def\exm{\noindent {\bf Примеры:}}


\def\Cb{\ovl{C}}
\def\ffi{\varphi}
\def\pa{\partial}
\def\V{\bf V}
\def\La{\Lambda}
\def\eps{\varepsilon}
\def\del{\delta}
\def\Del{\Delta}
\def\A{\EuScript{A}}
\def\lan{\left\langle }
\def\ran{\right\rangle}
\def\bar{\begin{array}}
\def\ear{\end{array}}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\thrm{\begin{thm}}
\def\ethrm{\end{thm}}
\def\dfn{\begin{defn}}
\def\edfn{\end{defn}}
\def\lm{\begin{lem}}
\def\elm{\end{lem}}
\def\zd{\begin{zad}}
\def\ezd{\end{zad}}
\def\prdl{\begin{predl}}
\def\eprdl{\end{predl}}
\def\crl{\begin{cor}}
\def\ecrl{\end{cor}}
\def\rm{\begin{rem}}
\def\erm{\end{rem}}
\def\fct{\begin{fact}}
\def\efct{\end{fact}}
\def\enm{\begin{enumerate}}
\def\eenm{\end{enumerate}}
\def\pmat{\begin{pmatrix}}
\def\epmat{\end{pmatrix}}

\frenchspacing
\righthyphenmin=2
%\usepackage{floatflt}
\captiondelim{. }





\begin{document}

\title{Конспект. Осень 2018}
\date{}
\author{}
\maketitle
\tableofcontents

\chapter{Полилинейная алгебра}
\section{Кватернионы}

Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим подпространство в алгебре матриц $M_2(\mb C)$ вида
$$\mb H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\mb C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\mb H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jji=i=-kj.$$ Таким образом $\mb H$ образует ассоциативную алгебру размерности 4 над $\mb R$.
 
\dfn[Алгебра кватернионов] $\mb H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно их и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\mb C$-алгебры.
\ezd



Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \mb R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm


\dfn[Сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и мнимую или векторную часть $v=\Im x= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn

\dfn[Норма кватерниона] Определим норму кватерниона как $$||x||=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \mb H$, то $x^{-1}=\frac{\ovl{x}}{||x||^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\mb R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. Мы не будем обсуждать неассоциатные алгебры в связи с тем, что им находится применение либо внутри физических дисциплин, либо внутри самой математики. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $||x||=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $||xy||=||x||||y||$. В частности, $||x^{-1}||=||x||^{-1}$.
\elm

\crl[Сумма четырёх квадратов] В любом коммутативном кольце произведение $(a^2+b^2+c^2+d^2)(e^2+f^2+g^2+h^2)$ снова есть сумма четырёх квадратов.
\ecrl

Теперь легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{||y||^2||x||^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{||xy||^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.


\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \mb R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -||u||^2v+ \lan u,v\ran u$$
2) $|| [u,v]||= ||u||||v|||\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -||u||^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь 
$$||[u,v]||^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)= ||u||^2||v||^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=||u||^2||v||^2 - \lan u,v\ran^2= ||u||^2||v||^2(1-\cos^2 \ffi)$$
\endproof

\dfn Обозначим за $\mb H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\mb H_{1}\to \GL_3(\mb R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\mb R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \mb H \to \mb H$ вида $y \to xyx^{-1}$ Прежде всего покажем, что мы получили ортогональное преобразование $\mb R^4$. Имеем
 $$||xvx^{-1}||=||v||.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\mb R^3$. Таким образом $L_x$ ограничивается на $\mb R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. 
$$xux^{-1}=(a+bv)u(a-bv)= (a+bv)(au-[u,bv])=a^2u -ab[u,v]+ab[v,u]- b^2[v,[v,u]]=(a^2-b^2)u-2ab[u,v]$$
$$x[u,v]x^{-1}=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=a^2[u,v]+abu-ab[u,v]v-b^2uv=(a^2-b^2)[u,v]+2abu $$
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\mb R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь некоторое применение кватернионов.



\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами. Для этого заметим, что с каждым самосопряжённым оператором $L$ на евклидовом пространстве можно связать билинейную симметричную  форму $\lan x,Ly\ran$ или, что эквивалентно, квадратичную форму $\lan x,Lx\ran$. Безусловно по квадратичной форме можно обратно восстановить оператор. 

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти $$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \frac{\lan L^*Lx,x\ran}{\|x\|^2}}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум квадратичной формы на сфере.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
 где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$ \lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $U\cap W=\{0\}$, так как на $W$ форма принимает значения меньше или равные $\lambda_k$, а на $U$ -- строго большие. Однако $\dim W=n-k+1$. Приходим к противоречию с подсчётом размерности пересечения. 
\endproof


Введём определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базиc $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm

\crl Пусть $U$ некоторое подпространство, а $q(x)=x^{\top} Ax$. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$ упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
 $$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$ Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 








\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in V$, где $V$ -- это евклидово пространство размерности $n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<Наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=L+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=L+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее.

Действительно, выпишем условие $\sum_{i=1}^s \rho(x_i-a_0, L)^2=\sum ||pr_{L^{\bot}} (x_i-a_0)||^2$  минимально.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{L^{\bot}} x_i + 2 pr_{L^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $L^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $L$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетовряют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $L$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, L)^2=\sum_{i=1}^s ||pr_{L^{\bot}} (x_i)||^2.$$

Воспользуемся тем, что $||x||^2=||pr_L x||^2+||pr_{L^{\bot}} x||^2$ или $||x||^2-||pr_L x||^2=||pr_{L^{\bot}} x||^2$. Тогда получаем, что
$$\sum_{i=1}^s ||pr_{L^{\bot}}(x_i)||^2=\sum_{i=1}^s ||x_i||^2-||pr_L x_i||^2$$
должно быть минимально. Тогда сумма $\sum_i ||pr_L x_i||^2$ должна быть максимальна.


Для того чтобы посчитать проекцию выберем в $L$ ортонормированный базис $u_1,\dots,u_k$. Тогда перепишем
$$\sum_{i=1}^s ||pr_L x_i||^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся какой. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d=Xu_j$ состоит из скалярных произведений  $\lan v_i, u_j\ran$. Тогда выражение $\lan d,d\ran = (u_jX^{\top})Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2$, то есть как раз тому, что участвует в нашей сумме. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо минимизировать выражение
$$\sum_{j=1}^k q(u_j).$$



Таким образом мы ищем максимум $\Tr q_{L}$ по всем подпространствам $L$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. 
Сформулируем теперь ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно определённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $L=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. В частности, в методе главных компонент возникает матрица $X^{\top}X$ и её собственные числа. Про них поговорим подробнее.

\dfn[Сингулярные значения] Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда сингулярными значениями $A$ называются числа $\sigma_i=\sqrt{d_i}$, где $d_i>0$ -- положительные собственные числа оператора $A^*A \colon V \to V$. Если же говорить на языке матриц, то для матрицы $X$ её сингулярными значениями будут корни из собственных чисел $X^{\top}X$. 
\edfn

На самом деле мы не обсуждали определение сопряжённого линейного отображения, а только сопряжённого оператора. Напишу немного об этом.

\dfn Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда сопряжённым отображением к $A$, называется такое линейное отображение $A^{*}$, что $\lan A^*x,y\ran = \lan x,Ay\ran$ для всех $x\in V$ и $y \in U$.
\edfn

\thrm Сопряжённое линейное отображение единственно. Более того, если в $U$ и $V$ выбрать ортонормированные базисы, то матрица сопряжённого отображения в этих базисах будет равна транспонированной матрице исходного.
\proof Достаточно доказать последнюю часть, чтобы показать единственность и существование. Выберем ортонормированные базисы в $U$ и $V$ -- $u_j$ и $v_i$. Обозначим матрицу $A$ в этом базисе за $X$, а кандидата на $A^*$ за $Y$. Тогда для равенства из определения сопряжённости необходимо и достаточно, его выполения на базисных. Иными словами необходимо и достаточно, чтобы $\lan X^{*}e_i,e_j\ran=\lan e_i,Xe_j\ran$. Но первая часть даёт $X^{*}_{ji}$, а вторая -- $X_{ij}$. Итого необходимо и достаточно, чтобы $X^{*}=X^{\top}$.   
\endproof
\ethrm

Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений.


\thrm[SVD разложение] Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $A$ имеет вид 
$$\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $A$, числа $\Sigma=\sigma_1, \dots, \sigma_r$ её сингулярные значения.
На языке матриц это означает, что для любой матрицы $X \in M_{m\times n}$ существуют матрицы $L$ -- размера $m$ и $R$ -- размера $n$,  что
$$X= L \Sigma R,$$
 с теми же условиями на $r$ и $\sigma_i$.
 
\proof Рассмотрим оператор $B = A^{*}A$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Ae_i \in U$. Они ортогональны. Действительно
$$\lan Ae_i, Ae_j\ran = \lan A^{*}Ae_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $||e_i||^2=d_i$. Возьмём 
$$f_i=\frac{Ae_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Ae_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $A$
Напоследок осталось решить вопрос, как выглядит матрица $R$. В нашей конструкции матрица $R$ есть матрица замены из стандартного базиса в базис из собственных векторов $e_i$ матрицы $X^{\top}X$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $R=C^{-1}$, но $C$ ортогональна и поэтому можно написать $R=C^{T}$, то есть строки $R$ -- собственные вектора $X^{\top}X$. Часто эти вектора называют правыми сингулярными векторами $X$.

\endproof
\ethrm

\zd Получите аналогичное описание для $L$. Покажите так же, что ничего кроме $\Sigma$ в аналогичном разложении получится не может.
\ezd

SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $L$, то получится матрица ранга $k$ или меньше. Эта матрица будет ближайшей к исходной в смысле вот такой вот матричной нормы, называемой, нормой Фробениуса 
$$||X||_F=\sqrt{\sum_{i,j} a_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$
Таким образом нахождение проекций точек можно переформулировать, как нахождение ближайшей к матрице $X$ матрице ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение

\thrm Пусть $X\in M_{m\times n}(\mb R)$. И SVD-разложение $X$ имеет вид $X=L\Sigma R$, где на диагонали $\Sigma$ стоят $\sigma_1,\dots,\sigma_r$ и нули. Тогда наилучшим приближением ранга $k$ в смысле нормы Фробениуса к матрице $X$ будет матрица $X^{(k)}=L\Sigma^{(k)}R$, где на диагонали $\Sigma^{(k)}$ стоят $\sigma_1,\dots,\sigma_{k}$ и нули.
\proof Для того, чтобы найти матрицу $X^{(k)}$ необходимо спроецировать строки $X$ на подпространство $L=\lan v_1^{\top},\dots,v_k^{\top}\ran$, где $v_i$ ортонормированный базис из собственных векторов $X^{\top}X$.  Вспомним, что строки $R$ есть $v_i^{\top}$. Для того, чтобы спроецировать одну строку $a$ на пространство $V^{(k)}$ необходимо вычислить сумму $\sum_{i=1}^k (av_i)v_i^{\top}$. Применив это целиком к матрице $X=L\Sigma R$ получим 
$$X^{(k)}=\sum_{i=1}^k Xv_iv_i^{\top}=L\Sigma \left(\sum_{i=1}^k Rv_iv_i^{\top}\right).$$
Вычислим последнюю сумму. Эта сумма считает проекции строк $R$ на $L$. Но первые $k$ строк лежат в $L$, а остальные ортогональны $L$. Итого имеем
$$R^{(k)}=\sum_{i=1}^k Rv_iv_i^{\top} = \pmat v_1^{\top} \\ \vdots \\ v_k^{\top} \\ 0 \\ \vdots \\ 0 \epmat.$$
Осталось заметить, что $\Sigma^{(k)} R= \Sigma^{(k)}R^{(k)}=\Sigma R^{(k)}$.
\endproof
\ethrm 

SVD-разложение используется в разных задачах, в том числе и для сжатия изображений.  Для простоты рассмотрим случай квадратного $n \times n$ чёрно белого изображения. Сделаем из него вещественную матрицу $X$ размера $n \times n$ и найдём SVD-разложение $L \Sigma K$. Тогда приближение $X^{(k)}$ задаётся $L\Sigma^{(k)}R$. Однако, как мы уже заметили, вместо матрицы $R$ можно взять матрицу $R^{(k)}$. Аналогично вместо $L$ можно взять $L^{(k)}$ -- выкинув из $L$ последние $n-k$ столбцов. Для хранения матрицы $\Sigma^{(k)}$ нужно $k$ параметров, для матриц $L^{(k)}$ и $R^{(k)}$ по $kn$ параметров. Итого нужно $2kn+k$ параметров. Однако чтобы не хранить отдельно $\Sigma$ её можно домножить на $L$ и хранить $L\Sigma$. В таком случае необходимо $2kn$ параметров. При $k<\frac{n}{2}$ это даёт эффект сжатия. 

Однако, это не предел. Посмотрим, сколько параметров нужно, чтобы задать $X$ -- матрицу ранга $k$. Пусть главный минор размера $k$ матрицы $X$ не ноль (априори мы знаем, что какой-то минор такого размера не ноль). Тогда для $j$-ой строки матрицы, начиная с номера $j \geq k+1$ есть набор чисел $a_{1,j},\dots,a_{k,j}$, которые есть коэффициенты в линейной комбинации дающей из первых строк $j$-ую. Аналогично для столбцов. Такой набор данных задаётся $k^2+ 2k(n-k)=2kn-k^2$ параметрами. Осталось заметить, что всегда $2kn - k^2\leq n^2$ так как $0\leq n^2-2kn+k^2=(n-k)^2$. Если невырожденным оказался не главный минор, то дополнительно нужно задать $2k$ дискретных параметров задающий номера строк и столбцов невырожденного минора.



\section*{Дополнительно: поиск угла между подпространствами}
Попробуем решить другую задачу -- определить, что такое угол угол между подпространствами и понять, как его найти. 

\dfn
Пусть $U$ и $W$ два подпространства  евклидового пространства. Определим косинус угла между ними как 
$$\cos \angle U,W= \sup_{\substack{ x\in U\\ y\in V}} \frac{\lan x,y\ran}{||x|| ||y||}.$$
\edfn

\rm Если два подпространства пересекаются, то угол между ними по этому определению равен 0. Если хочется, чтобы угол не был равен 0 для неравных пространств, то разумно посмотреть ортогональные дополнения $U$ и $W$ к пересечению $U\cap W$ и посчитать угол между ортогональными дополнениями. 
\erm

\rm Можно переписать выражение для косинуса как $$\cos \angle U,W= \sup_{ x\in U} \cos \angle x, V = \sup_{x\in U} \frac{||pr_V x||}{||x||}= \sqrt{ \sup_{\substack{x\in U\\ ||x||=1}} ||pr_V x||^2} .$$
\erm

С последним выражением легко работать, потому что $||pr_V x||^2$ -- это квадратичная форма на $U$.

Допустим мы хотим максимизировать указанное выражение. Выберем на $U$ ортонормированный базис. Посчитаем матрицу формы $||pr_V x||^2$ по формуле 
$$a_{ij}= \lan pr_V u_i, pr_V u_j\ran.$$

Тогда максимум выражения под корнем равен $\lambda_{max}$ -- максимальному собственному числу $A$ и достигается на соответствующем векторе $v_{max}$. Ответ: $\sqrt{\lambda_{max}}$

\rm Получилось, что косинус угла -- это норма оператора проекции с одного подпространства на другое. Это не совсем то, что нас интересует в общем случае, например для гиперплоскостей. Дело в том, что для них ответ всегда 1, так как гиперплоскости заведомо пресекаются по подпространство ненулевой размерности (кроме как на плоскости). Вектора из пересечения проецируются в себя,что даёт норму равную единице. В этом случае стоит заменить оба подпространства на их ортогональные дополнения к пересечению. Это тоже самое, что найти первое неединичной собственное числа для формы  $\lan pr_V u_i, pr_V u_j\ran$ на $U$.
\erm




\section{Спектры графов}

\dfn
Для каждого графа $G$ можно построить  несколько  различных матриц, которые кодируют его структуру. Прежде всего это три квадратные матрицы  размера $n\times n$, где $n$ -- это число вершин $G$. 
Первая -- матрица смежности  $A(G)$, которая полностью определяет граф $G$
$$a_{ij}=\begin{cases} 1, \text{ если вершины $i$ и $j$ соединены ребром}\\
0, \text{ иначе }
\end{cases}.$$

Так же нам уже встречалась матрица случайного блуждания  $P(G)$

$$P_{ij}=\begin{cases}
\frac{1}{d_j}, \text{ если есть ребро $j\to i$}\\
1, \text{ если из вершины не исходит рёбер} \\
0, \text{ иначе }
\end{cases}.$$
Кроме того, полезна бывает матрица инцидентности $B(G)$ размера $n\times m$, где $m$ -- число рёбер.
\edfn

В прошлом семестре мы с вами поняли, что для понимания того, как устроен предел последовательности $P^nv$, необходимо представлять себе как устроены собственные числа матрицы $P$. Прежде всего мы с вами понимали, что у матрицы $P$ есть собственное число 1. Однако встаёт несколько вопросов:\\
1) Какова кратность единицы, как собственного числа?\\
2) Есть ли другие собственные числа, равные единице по модулю?\\
3) Если $Pv=v$, то мы хотели бы интерпретировать $v$ как вектор весов для вершин графа. Верно ли, что $v$ можно выбрать положительным? Сколько таких независимых $v$?

Понятно, что в общем случае ответ на первые два вопроса <<нет>>.

\exm \\
1) Рассмотрим граф 
\begin{center}
\begin{tikzpicture}

\begin{comment}
\draw [fill] (0,0) circle [radius=0.05];
\draw [fill] (1,0.5) circle [radius=0.05];
\draw [fill] (1,-0.5) circle [radius=0.05];
\end{comment}

\node (A) at (0,0) {3};
\node (B) at (1,0.5) {1};
\node (C) at (1,-0.5) {2};
\path[->,font=\scriptsize,>=angle 60]
(A) edge (B)
(A) edge (C);
\end{tikzpicture}
\end{center}
У его матрицы $P$ очевидно есть два собственных вектора $(1,0,0)$ и $(0,1,0)$ с собственным числом 1.\\
2) Рассмотрим граф $C_n$ -- цикл длины $n$. Его спектр -- это корни степени $n$ из единицы.\\

Сейчас мы докажем, что при некоторых предположениях на матрицу для неё ответы на все три вопроса оказываются положительными. Эти предположения не будут выполнены для матриц $A(G)$ и  $P(G)$ непосредственно, однако мы тем не менее сможем извлечь пользу.

\dfn Назовём матрицу $A$ положительной (не путать с положительно определённым оператором), если все её элементы $A_{ij}$ строго положительны. Будем писать в этом случае $A>0$.
\edfn

\dfn Назовём матрицу  $A$ не отрицательной, если $A_{ij}\geq 0$. Обозначение $A \geq 0$.
\edfn

\thrm[Перрон, 1907] Если матрица $A$ положительна, то наибольшее по модулю собственное число $A$ единственное и является вещественным и положительным. Это собственное число не является кратным корнем характеристического многочлена. Собственный вектор для этого собственного числа положителен.
\ethrm
\proof Пусть $\lambda$ -- наибольшее по модулю собственное число и $Ax=\lambda x$. Можно считать, что $|\lambda|=1$. Тогда покажем, что $A|x|=|x|$.

Прежде всего мы имеем цепочку неравенств $|x|=|Ax|\leq |A||x|=A|x|$, где все неравенства подразумеваются покомпонентными. Обозначим за $z=A|x|$. Это вектор состоящий из положительных координат и рассмотрим вектор $y=z-x$. Вектор $y$ неотрицателен. При этом если $y=0$, то мы доказали то, что хотели. Предположим, что есть координата $y_i>0$. Тогда $Ay$ -- положительный вектор, то есть существует такое $\eps>0$, что $Ay>\eps z$. Распишем это неравенство: $Az - z= Az-A|x|> \eps z$ или же $\frac{A}{1+\eps}z>z$. Ввиду положительности правой и левой части мы без сомнений можем применить оператор $\frac{A^n}{(1+\eps)^n}$ к правой и левой части и получить верное неравенство. Итого имеем цепочку 
$$\frac{A^n}{(1+\eps)^n}z>\frac{A^{n-1}}{(1+\eps)^{n-1}}> \dots > z.$$
Но оператор $\frac{A}{1+\eps}$ имеет собственные числа по модулю меньшие 1 и поэтому, как мы знаем с прошлого семестра, предел левого выражения равен 0. Противоречие!

Итак, в частности, единица собственное число. Покажем теперь, что нет отличных от единицы собственных чисел. Пусть $\lambda$ собственное число $A$ с $|\lambda|=1$ и $x$ -- соответствующий собственный вектор. Тогда $A|x|=|x|=|Ax|$. Заметим, что все координаты $x$ отличны от нуля. Рассмотрим $i$-ую координату. Имеем $\sum A_{ij}|x_j|=x_i=|\sum A_{ij}x_j|$. Посмотрим на это равенство как на равенство нормы векторов в $\mb R^2$. Хорошо известно, что сумма норм больше или равна нормы суммы и равенство достигается тогда и только тогда, когда вектора сонаправлены. Итого координаты $x_i$ должны быть сонаправлены, но это означает, что $x=e^{i\ffi} |x|$ и следовательно $\lambda=1$. 

Покажем, что единица не кратный корень. Действительно, пусть есть два собственных вектора $x_1$ и $x_2$. Тогда подберём $c$, так что $x_1-cx_2$ имеет нулевую координату. Получаем противоречие, так как $|x_1-cx_2|$ неотрицательный вектор для 1, но при этом с нулевой координатой. Осталось разобрать случай, когда $x_2$ присоединён к $x_1>0$, то есть $Ax_2=x_2+x_1$. Тогда имеем $A^nx_2=x_2 +nx_1$. Это значит, что какие-то коэффициенты $A^n$ растут по крайней мере линейно по $n$. Но тогда и коэффициенты $A^nx_1=x_1$ тоже растут по крайней мере линейно, что очевидно не так.
\endproof

Так же бывает полезно ещё одно утверждение. 
\lm Пусть $A>0$, $\lambda$ -- максимальное по модулю собственное число. Если у матрицы $A$ есть собственный вектор $y\geq 0$, то $y$ собственный вектор для числа $\lambda$
\elm
\proof Рассмотрим матрицу $A^{\top}$. У неё есть положительный  собственный вектор $x$, соответсвующий собственному числу $\lambda$. Пусть $\mu$ -- собственное число для $y$. Тогда 
$$\lambda x^{\top}y= x^{\top}Ay=x^{\top}\mu y=\mu x^{\top}y.$$
Так как $x^{\top}y >0$, то $\lambda=\mu$.
\endproof


Вообще говоря матрица $P(G)$ имеет довольно много нулевых компонент. И строго говоря теорема Перрона не может быть верна для $P(G)$. Как же она может помочь? Для этого мы схитрим и немного поменяем задачу. А именно, рассмотрим матрицу $$P_{\alpha}(G)=(1-\alpha) P(G) + \alpha\tfrac{1}{n}J_n,$$
где $J_n$ -- матрица из одних единиц, а $\alpha \in (0,1)$. Тогда матрицы $P_{\alpha}(G)$ являются положительными. С точки зрения блуждающего пользователя это означает, что у него есть два режима -- первый, в котором он находится с вероятностью $1-\alpha$ -- это режим брожения по ссылкам, а второй режим -- переход на случайную страницу. Для матрицы $P_{\alpha}(G)$ выполнены условия теоремы и поэтому она имеет единственное не кратное максимальное собственное число, которое положительно и соответствующий собственный вектор положителен. Покажем, что это собственное число равно 1.

Для этого рассмотрим матрицу $P_{\alpha}(G)^{\top}$. У этой матрицы есть положительный собственный вектор $(1,\dots,1)$ с собственным числом 1. Но тогда это максимальное по модулю собственное число для $P_{\alpha}(G)^{\top}$ и следовательно для $P_{\alpha}(G)$. Изучая предел $P_{\alpha}(G)$ при $\alpha \to 0$ можно получить информацию и про исходную матрицу.

То, что у $P_{\alpha}(G)$ все собственные числа по модулю меньше единицы означает, что $P_{\alpha}(G)^kv \to cx$, при $k \to \infty$, где $x$ -- положительный вектор с собственным числом равным 1. Это позволяет приближённо найти $x$, что даёт желаемое распределение весов. Практически для этого можно взять $k\sim \log n$. Это позволяет заметно сэкономить на вычислениях по сравнению с теоретическим нахождением собственных векторов.


Теперь переключимся на основной случай, который будем рассматривать -- случай неориентированных графов. В этой ситуации особую роль играет матрица $A(G)$.


\dfn Спектр графа -- это спектр его матрицы смежности $A(G)$.
\edfn


Для начала разберёмся с оценками и свойствами собственных чисел матрицы смежности. Здесь нам пригодится теорема Перрона. 


\lm Пусть граф $G$ связен. Тогда его максимальное собственное число положительно, не кратно и соответствующий собственный вектор имеет положительные координаты. Более того, все собственные числа графа по модулю меньше чем максимальная степень $d_{max}$. Граф $G$ регулярен тогда и только тогда, когда $d_{max}$ -- это его собственное число.
\elm
\proof Прежде всего отметим, что все собственные числа $G$ вещественные и максимальное собственное число положительно так как $\Tr A(G)=0$. Рассмотрим теперь матрицу $(A+\eps I)^{n-1}$. Это положительная матрица. Действительно в $(A+\eps I)^{n-1}_{ij}$ входит слагаемое $\eps^{n-1-l}$, где $l$ -- длина пути между $i$ и $j$. То же можно сказать и про большую степень $A+\eps I$. Максимальное с.ч. $A$ соответствует максимальному с.ч. $(A+\eps I)^l$ по крайней мере, если $\eps$ очень большое. Но тогда соответствующий собственный вектор $v$ положителен и максимальное собственное число $A+\eps I$ и, следовательно, $A$ не кратно. Далее $v$ положительный собственный вектор для всех $(A+\eps I)^{l}$, откуда получаем, что максимальное с.ч. у всех $(A+\eps I)^{l}$ наибольшее по модулю $(\lambda_1+\eps)^l>|\lambda_i+\eps|^l$. Переходя к пределу при $\eps \to 0$ получаем, что $\lambda_1^l \geq |\lambda_i|^l$. Осталось извлечь корень.

Почему же $\lambda_1 \leq d_{max}$? Пусть $x$ -- собственный вектор для числа $\lambda_1$. Тогда $Ax=\lambda_1 x$. Посмотрим, насколько мог измениться $x$ при домножении на $A$. Рассмотрим максимальную координату $x_i$. Имеем $\lambda_1 x_i= \sum a_{ij}x_j\leq d_{max} x_i$.

Предположим, что $d_{max}$ собственное число. Тогда в указанном выше неравенстве достигается равенство, то есть $x_i=x_j$ для соседних вершин. Но это значит, что вектор $(1,\dots,1)$ собственный, что бывает только в случае регулярного графа. 
\endproof


\exm \\
1) Спектр полного графа $K_n$ равен $n-1$ , $-1, \dots,-1$.\\
2) Спектр цикла длины $n$ равен $2\cos(\frac{2\pi l}{n})$.\\
Попробуем разобрать ещё один пример:

\dfn Сильно регулярный граф с параметрами $n$, $k$, $\lambda$ и $\mu$ это $k$-регулярный граф на $n$ вершинах, такой что любые две смежные вершины имеют $\lambda$ соседей, а любые две несмежные -- $\mu$ соседей.
\edfn

\thrm Матрица сильно $k$-регулярного графа удовлетворяет соотношению $A^2+(\mu-\lambda)A + (\mu-k)E=\mu J$, где $J$ -- это матрица из одних единиц.
\ethrm
\proof Возведём матрицу $A$ в квадрат. Тогда, число общих соседей равно числу путей длины 2 из $i$ в $j$, то есть $A^2_{ij}$. Если $i,j$ связаны между собой, то $A^2_{ij}=\lambda$, если не связаны, то $A^2_{ij}=\mu$, а на диагонали стоит $k$. Прежде всего вычтем $kE$, что даст нули на диагонали. Вычтя $\lambda A$ получим нули в тех позициях, что соответствовали рёбрам. Теперь надо добавить $\mu A+ \mu E$, чтобы эти позиции заполнить мюшками и получить $\mu J$. 
\endproof

Эта теорема позволяет легко посчитать спектры 
\crl
 Граф Петерсена сильно регулярный. Его спектр 3, 1, 1, 1, 1, 1, -2, -2, -2, -2.
\ecrl



Посмотрим, какие свойства графа можно увидеть благодаря его спектру.

\lm  След степени матрицы смежности считает количество циклов (возможно с пересечениями). В частности, граф двудольный тогда и только тогда, когда его спектр симметричен. 
\elm



Попробуем понять, какую ещё информацию даёт спектр. Воспользовавшись следствием из теоремы Куранта-Фишера получаем:

\thrm Пусть $G$ -- граф на $n$ вершинах. Пусть $A$ -- симметричная матрица $n\times n$, такая, что $A_{ij}= 0$, если вершины не соединены ребром. Пусть $n_{+}$ и $n_{-}$ количество положительных и отрицательных собственных чисел $A$. Тогда размер независимого множества в $G$ не превосходит $\min(n-n_{+},n-n_{-})$.
\ethrm
\proof Действительно, если взять подпространство, натянутое на вершины из независимого множества, то ограничение формы $x^{\top}Ax$ будет нулевым. Такое бывает только на подпространстве размерности $n-n_{+}=n_{-}+n_0$ исходя из нижней оценки. Аналогично получаем второе неравенство.
\endproof

Однако это не единственная возможная оценка. Разберём пример, использующий понятие спектра, но основанный на совершенно других предположениях.



\thrm Пусть $G$ -- $k$-регулярный граф. Тогда размер максимального независимого множества в графе оценивается как $$\alpha(G)\leq -\frac{\lambda_n}{k-\lambda_n}.$$
\ethrm
\proof Рассмотрим характеристический вектор $v$ для независимого множества размера $\alpha$. Имеем $v^{\top}Av=0$ и при этом $v^{\top}v=\alpha$. Так как граф $k$-регулярный, то у него есть нормированный собственный вектор $u_1=\frac{1}{\sqrt{n}}(1,\dots,1)$. Тогда $\lan v,u_1\ran = \frac{\alpha}{\sqrt{n}}$. Разложим вектор $v=c_1u_1 + \dots + c_n u_n$ по ортонормированной системе собственных векторов. Тогда
$$0=v^{\top}Av=\sum c_i^2 \lambda_i= \lambda_1\frac{\alpha^2}{n}+ \sum_{i\geq 2} \lambda_i c_i^2\geq \lambda_1\frac{\alpha^2}{n}+ \lambda_n \sum_{i\geq 2} c_i^2.$$
Мы знаем, что $\sum c_i^2=\alpha$, откуда $\sum_{i\geq 2} c_i^2=\alpha - \frac{\alpha^2}{n}$. Итого 
$$0\geq \lambda_1\frac{\alpha^2}{n}+\lambda_n(\alpha- \frac{\alpha^2}{n})$$
Сокращая на $\alpha$ получаем 
$$(\lambda_1-\lambda_n)\frac{\alpha}{n}\leq -\lambda_n,$$
что, очевидно, эквивалентно нужному неравенству.
\endproof

Обе оценки дают для графа Петерсена $\alpha(G)\leq 4$, что является точной оценкой. Разберём ещё один пример теоремы, которая хотя и не является сверхсодержательной, показывает, как можно использовать спектр графа.

\thrm Рассмотрим граф $K_{10}$. Тогда его невозможно покрыть тремя копиями графа Петерсона.
\ethrm

\section{Тензоры}

В прошлом семестре мы подробно остановились на билинейных операциях и их связи с геометрией, но не разобрали в достаточной степени, что же происходит с общими полилинейными отображениями.

\dfn Пусть есть набор пространств $V_1, \dots,V_n$. Тогда их тензорным произведением называется пространство 
$V_1\otimes \dots \otimes V_n$ вместе с полилинейным отображением
$$i \colon V_1 \times \dots \times V_n \to V_1 \otimes \dots \otimes V_n,$$
удовлетворяющее условию что для любого билинейного отображения из $h\colon V_1\times \dots \times V_n \to U$ существует единственное линейное отображение 
$$\hat{h}\colon V_1\otimes \dots \otimes V_n \to U,$$
что 
$$\hat{h}\circ i=h.$$
Иными словами, отображение $i$ -- это <<универсальное>> полилинейное отображение.
\edfn 


\lm Если тензорное произведение существует, то оно единственно.
\elm


Однако, совершенно непонятно есть такое пространство или нет. Более того, самом его существование нам тоже не сильно поможет. Нам нужна конструкция этого пространства и понимание свойств этой конструкции.

\thrm Пусть $V_1,\dots,V_n$ -- набор векторных пространств. Тогда имеет место следующая конструкция тензорного произведения:
$$V_1 \otimes \dots \otimes V_n \cong K\lan V_1 \times \dots \times V_n \ran / Rel,$$
где $Rel$ -- это подпространство порождённое формальными суммами
$$(\dots, \lambda v_1+v_2, \dots) - \lambda (\dots,v_1, \dots) - (\dots, v_2, \dots).$$ 
\ethrm

\dfn Будем обозначать элемент $i(v_1,\dots,v_n)=v_1\otimes \dots \otimes v_n$. 
\edfn

Теперь необходимо посчитать что-то про тензорное произведение. Например, научиться считать размерность тензорного произведения и находить его базис.
\thrm Пусть $e_{i1},\dots,e_{in}$ базис $V_i$. Тогда $$e_{1j_1}\otimes \dots \otimes e_{nj_n}$$ базис $V_1 \otimes \dots \otimes V_n$.
\ethrm



Определим теперь тензорное произведение линейных отображений.

\dfn Пусть набор линейных отображений $f_i \colon U_i \to V_i$. Определим отображение $$f_1\otimes \dots \otimes f_n \colon \otimes U_i \to \otimes V_i$$ по  правилу $$(f_1\otimes \dots \otimes f_n) (u_1\otimes \dots \otimes u_n) = f(u_1)\otimes \dots \otimes f(u_n).$$
Отображение с таким свойством единственно.
\edfn

\rm Указанное отображение корректно задано. Действительно,  отображение $f_1\otimes \dots \otimes f_n$ должно быть продолжением отображения $(u_1,\dots,u_n) \to f(u_1) \otimes \dots \otimes f(u_n)$, которое очевидно полилинейно. Тогда такое продолжение существует и единственно по свойству тензорного произведения. 
\erm

\rm Пусть заданы наборы линейных отображений $f_i$ и $g_i$, так что определены композиции $f_i\circ g_i$. Тогда $$f_1\otimes \dots \otimes f_n \circ g_1\otimes \dots \otimes g_n=(f_1\circ g_1)\otimes \dots \otimes (f_n\circ g_n)$$
\erm 

А как устроена матрица тензорного произведения линейных отображений?


\lm Пусть $L_1 \colon V_1 \to U_1$, а $L_2 \colon V_2 \to U_2$. Пусть $e_1,\dots, e_{n_1}$ базис $V_1$,  $e_1',\dots, e_{n_2}'$ базис $V_2$,  и $f_1,\dots, f_{m_1}$ -- базис $U_1$, а $f_1',\dots, f_{m_2}'$ -- базис $U_2$. 
Упорядочим базисы тензорных произведений -- удобно это сделать, например, в лексикографическом порядке (номер первой координаты важнее).
Тогда матрица  $L_1\otimes L_2$  разобьётся на $n_1m_1$ блоков в каждом из которых будет стоять $ A_{ij} B$, где $i,j$ -- номер блока, а $A$ и $B$ матрицы $L_1$ и $L_2$ соответственно.
\elm

\dfn Такая матрица называется кронекеровым произведением матриц $A$ и $B$ и обозначается как $A\otimes B$.
\edfn

А что если стартовать с операторов, а не с линейных отображений?

\rm Если есть операторы $A\colon V \to V$ и $B \colon W \to W$. Тогда задан оператор $A\otimes B$ на $V\otimes W$. \erm

\lm У оператора $A\otimes B$ собственные числа -- это попарные произведения с.ч. для $A$ и $B$. 
\elm


 

\dfn[Произведение графов] Пусть $G$ и $H$ -- два графа(возможно ориентированных). Тогда их категорным произведением называется граф чьи вершины есть пары вершин $G$ и $H$ и ребро между парами $(u_1,v_1)$ и $(u_2,v_2)$ проводится только если есть рёбра $u_1 \to u_2$ и $v_1 \to v_2$. Декартовым произведением графов $G$ и $H$ называется граф на тех же вершинах с ребром между парами если $u_1=u_2$ и есть ребро $v_1\to v_2$ или, симметрично, $v_1=v_2$ и есть ребро $u_1 \to u_2$. Разумеется для неориентированных графов эта конструкция снова выдаёт неориентированный граф.
\edfn

\rm Матрица произведения графов -- это тензорное произведение матриц.
\erm

\crl Спектр категорного произведения графов состоит из всех возможных попарных произведений собственных чисел графов.
\ecrl

\zd Чему равен спектр декартового произведения графов?
\ezd


С понятием тензорного произведения связан ряд канонических отождествлений между разными на первый взгляд пространствами в духе изоморфизма $V \sim V^{**}$.

\thrm Имеют место следующие естественные изоморфизмы: 
$$(U \otimes V) \otimes W \cong U \otimes V \otimes W \cong U \otimes (V \otimes W)$$
$$ U \otimes V \cong V \otimes U $$
$$ \Hom (U,V) \cong V \otimes U^*$$
$$ \Hom (U\otimes V,  W) \cong \Hom (U, \Hom (V,W))$$
$$(U \otimes V)^{*} \cong U^{*}\otimes V^{*}$$
\ethrm










\end{document}